{"paragraphs":[{"text":"%md\n# Pretty Printing\n\nCopy & Paste your solution for formatting DataFrames for Zeppelin","dateUpdated":"Jun 11, 2016 4:48:18 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665414_-773804887","id":"20160611-164745_1805445937","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2185","dateFinished":"Jun 11, 2016 4:48:19 PM","dateStarted":"Jun 11, 2016 4:48:19 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Pretty Printing</h1>\n<p>Copy &amp; Paste your solution for formatting DataFrames for Zeppelin</p>\n"},"focus":true},{"text":"// Put your code here\n","dateUpdated":"Jun 11, 2016 4:49:05 PM","config":{"enabled":true,"graph":{"mode":"table","height":294,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"rank","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"rank","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665414_-773804887","id":"20160611-164745_185931049","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2186"},{"text":"%md\n# 1. Load Data\nWe load the data that we have prepared in the last exercise. Only you know where you have stored it, so don't ask me :)","dateUpdated":"Jun 11, 2016 4:48:31 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665415_-774189636","id":"20160611-164745_354434158","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2187","dateFinished":"Jun 11, 2016 4:48:31 PM","dateStarted":"Jun 11, 2016 4:48:31 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>1. Load Data</h1>\n<p>We load the data that we have prepared in the last exercise. Only you know where you have stored it, so don't ask me :)</p>\n"},"focus":true},{"text":"// Load previously generated data into a DataFrame called \"sessions\"\nval sessions = ...\n\nsessions.printSchema()","dateUpdated":"Jun 11, 2016 4:49:05 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665415_-774189636","id":"20160611-164745_2002740375","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2188"},{"text":"%md\n# 2. Create Indices usable for GraphX\nIn GraphX all Vertices are identified by 64bit Long values. But of course, our data is String based. Therefore we need to create an Index, which assigns a unique ID to every value which is to be used as a Vertex.\n\nWe will use both the sessions and the linking keys (cdk) for vertices, so we need to create a common index for both key types.\n\nSo what we eventually want to do is the following:\n1. Get all session keys from the data\n2. Get all linking keys (cdk) from the data\n3. Union these keys together (they are prefixed with sesn_ and cdk_, so they won't interfere)\n4. Make set of keys distinct\n5. Assign unique ID to each key\n\nIt turns out that the last step is a bit more tricky than expected due to the nature of RDDs. They might be reevaluated at any time, and that might happen even only partially. From the documentation of 'zipWithUniqueId':\n\n    Note that some RDDs, such as those returned by groupBy(), do not guarantee order of elements in a partition. The unique ID assigned to each element is therefore not guaranteed, and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee the same index assignments, you should sort the RDD with sortByKey() or save it to a file. \n    \nWe take this warning very serious and will sort the data before crteating unique IDs. So there will be an additional step, and our receipe looks as follows\n1. Get all session keys from the data\n2. Get all linking keys (cdk) from the data\n3. Union these keys together (they are prefixed with sesn_ and cdk_, so they won't interfere)\n4. Make set of keys distinct\n5. Sort keys\n6. Assign unique ID to each key\n","dateUpdated":"Jun 11, 2016 4:48:58 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665415_-774189636","id":"20160611-164745_398799547","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2189","dateFinished":"Jun 11, 2016 4:48:58 PM","dateStarted":"Jun 11, 2016 4:48:58 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>2. Create Indices usable for GraphX</h1>\n<p>In GraphX all Vertices are identified by 64bit Long values. But of course, our data is String based. Therefore we need to create an Index, which assigns a unique ID to every value which is to be used as a Vertex.</p>\n<p>We will use both the sessions and the linking keys (cdk) for vertices, so we need to create a common index for both key types.</p>\n<p>So what we eventually want to do is the following:</p>\n<ol>\n<li>Get all session keys from the data</li>\n<li>Get all linking keys (cdk) from the data</li>\n<li>Union these keys together (they are prefixed with sesn<em> and cdk</em>, so they won't interfere)</li>\n<li>Make set of keys distinct</li>\n<li>Assign unique ID to each key</li>\n</ol>\n<p>It turns out that the last step is a bit more tricky than expected due to the nature of RDDs. They might be reevaluated at any time, and that might happen even only partially. From the documentation of 'zipWithUniqueId':</p>\n<pre><code>Note that some RDDs, such as those returned by groupBy(), do not guarantee order of elements in a partition. The unique ID assigned to each element is therefore not guaranteed, and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee the same index assignments, you should sort the RDD with sortByKey() or save it to a file. \n</code></pre>\n<p>We take this warning very serious and will sort the data before crteating unique IDs. So there will be an additional step, and our receipe looks as follows</p>\n<ol>\n<li>Get all session keys from the data</li>\n<li>Get all linking keys (cdk) from the data</li>\n<li>Union these keys together (they are prefixed with sesn<em> and cdk</em>, so they won't interfere)</li>\n<li>Make set of keys distinct</li>\n<li>Sort keys</li>\n<li>Assign unique ID to each key</li>\n</ol>\n"},"focus":true},{"text":"import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\n\n\n// Get session keys from data, add one more column marking this key as being a session key\nval sessionKeys = sessions.select($\"session\" as \"key\", lit(true) as \"is_sessionkey\")\n// Get device keys from data, add one more column marking this key as not being a session key\nval deviceKeys = sessions.select($\"cdk\" as \"key\", lit(false) as \"is_sessionkey\")\n// Put both sets of keys together\nval keys = sessionKeys.unionAll(deviceKeys).distinct()\n\n// Create stable unique IDs. It is important to sort the data before calling zipWithIndex!\nval rows =\n    keys.rdd\n        .sortBy(row => row.getString(0))\n        .zipWithUniqueId()\n        .map(row_id => Row(row_id._1(0), row_id._1(1), row_id._2))\n\n// Recreate RDD from indexed data\nval indexSchema = StructType(\n    StructField(\"key\", StringType)\n        :: StructField(\"is_sessionkey\", BooleanType)\n        :: StructField(\"index\", LongType)\n        :: Nil\n)\nval indexedKeys = sqlContext.createDataFrame(rows, indexSchema)\n","dateUpdated":"Jun 11, 2016 4:47:45 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"key","index":0,"aggr":"sum"}],"values":[{"name":"is_sessionkey","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"key","index":0,"aggr":"sum"},"yAxis":{"name":"is_sessionkey","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665415_-774189636","id":"20160611-164745_327524142","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2190"},{"text":"indexedKeys.toZeppelin(10)","dateUpdated":"Jun 11, 2016 4:47:45 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665415_-774189636","id":"20160611-164745_697667333","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2191"},{"text":"%md\n## Create Vertices\n\nNow we want to create vertices for GraphX. What is the set of vertices? Actually it's the whole set of keys, so every entry in our DataFrame 'indexedKeys' should be added to vertices. And that is pretty easy, since the required VertexID is just the 'index' column.","dateUpdated":"Jun 11, 2016 4:49:06 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665416_-776113380","id":"20160611-164745_434153111","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2192","dateFinished":"Jun 11, 2016 4:49:06 PM","dateStarted":"Jun 11, 2016 4:49:06 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Create Vertices</h2>\n<p>Now we want to create vertices for GraphX. What is the set of vertices? Actually it's the whole set of keys, so every entry in our DataFrame 'indexedKeys' should be added to vertices. And that is pretty easy, since the required VertexID is just the 'index' column.</p>\n"},"focus":true},{"text":"// Create vertices from the indexedKeys. Map from every entry the 'index' to the vertex ID and the 'key' to be the vertex value. The result should be a RDD with entries (index, key)\nval vertices = ...","dateUpdated":"Jun 11, 2016 4:49:45 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665416_-776113380","id":"20160611-164745_1809387718","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2193"},{"text":"%md\n## Create Edges\n\nNow that we have the vertices, we also need the Edges. Since the Edges actually make up the topology of the graph, they are obviously a little bit more complex than vertices. We want to create an Edge for every connection between a session and a linking key (i.e. cdk). We'll do that with the following approach:\n\n1. Join indexedKeys to sessions, replacing session key by the appropriate 'index' entry in the indexedKeys\n2. Join indexedKeys to the result, now replacing the linking key (cdk) by the corresponding 'index' entry in indexedKeys\n3. Now you should have a DataFrame or RDD with two Long values, one for the session and one for the cdk.\n4. Each of these tuples need to be mapped to a GraphX Edge(cdkIndex, sessionIndex, 0)","dateUpdated":"Jun 11, 2016 4:49:39 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665416_-776113380","id":"20160611-164745_2010934906","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2194","dateFinished":"Jun 11, 2016 4:49:37 PM","dateStarted":"Jun 11, 2016 4:49:37 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Create Edges</h2>\n<p>Now that we have the vertices, we also need the Edges. Since the Edges actually make up the topology of the graph, they are obviously a little bit more complex than vertices. We want to create an Edge for every connection between a session and a linking key (i.e. cdk). We'll do that with the following approach:</p>\n<ol>\n<li>Join indexedKeys to sessions, replacing session key by the appropriate 'index' entry in the indexedKeys</li>\n<li>Join indexedKeys to the result, now replacing the linking key (cdk) by the corresponding 'index' entry in indexedKeys</li>\n<li>Now you should have a DataFrame or RDD with two Long values, one for the session and one for the cdk.</li>\n<li>Each of these tuples need to be mapped to a GraphX Edge(cdkIndex, sessionIndex, 0)</li>\n</ol>\n"},"focus":true},{"text":"import org.apache.spark.graphx.Edge\n\n// 1. Replace session keys by unique ID in indexed keys, keep CDK for next step\n\n// 2. Replace CDK by unique ID in indexed keys, keep ID from previous step\n\n// 3. Now you should have a DataFrame with columns for the ID of session and the ID for CDK\n// 4. Map all entries to Edge(session_id, cdk_id)\nval edges = ...","dateUpdated":"Jun 11, 2016 4:50:21 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665416_-776113380","id":"20160611-164745_685328656","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2195"},{"text":"%md\n## Create Graph\n\nNow we have everything together, we can create a GraphX Graph. This is now trivial, only the constructor needs to be called.","dateUpdated":"Jun 11, 2016 4:50:14 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665416_-776113380","id":"20160611-164745_1363922531","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2196","dateFinished":"Jun 11, 2016 4:50:14 PM","dateStarted":"Jun 11, 2016 4:50:14 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Create Graph</h2>\n<p>Now we have everything together, we can create a GraphX Graph. This is now trivial, only the constructor needs to be called.</p>\n"},"focus":true},{"text":"import org.apache.spark.graphx.Graph\n\nval sessionGraph = Graph(vertices, edges)","dateUpdated":"Jun 11, 2016 4:47:45 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665416_-776113380","id":"20160611-164745_1205530439","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2197"},{"text":"%md\n## Find Connected Components\n\nNow comes the next interesting step. We want to find all connected components in our Graph. This can be done using some simple functionality provided by GraphX called ConnectedComponents. It will return a new Graph with the vertex properties (i.e. the value attached to each Vertex) replaced by an ID identifying the component. The ID is selected as follows (from documentation):\n\n    Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex. ","dateUpdated":"Jun 11, 2016 4:50:18 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665417_-776498129","id":"20160611-164745_1989362475","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2198","dateFinished":"Jun 11, 2016 4:50:18 PM","dateStarted":"Jun 11, 2016 4:50:18 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Find Connected Components</h2>\n<p>Now comes the next interesting step. We want to find all connected components in our Graph. This can be done using some simple functionality provided by GraphX called ConnectedComponents. It will return a new Graph with the vertex properties (i.e. the value attached to each Vertex) replaced by an ID identifying the component. The ID is selected as follows (from documentation):</p>\n<pre><code>Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex.\n</code></pre>\n"},"focus":true},{"text":"import org.apache.spark.graphx.lib.ConnectedComponents\n\nval connectedGraph = ConnectedComponents.run(sessionGraph)","dateUpdated":"Jun 11, 2016 4:47:45 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665417_-776498129","id":"20160611-164745_1042217182","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2199"},{"text":"%md\n# Map Back Session Keys\n\nNow we have a new graph, where each Vertex has an ID which maps to the original session key or linking key. The value of each Vertex denotes which component the Vertex (and therefore session or linking key) belongs to. By looking up all original session keys and keeping the component ID, we can find all sessions which belong together (those session keys having the same component ID belong to the same user).\n\nSo our plan looks as follows:\n1. Convert vertices from Graph back into a DataFrame with two columns 'original_idx' and 'component_id'\n2. Join indexedKeys to the DataFrame on original_idx === index, look up session key\n3. Keep only session keys (so we can put sessions together. Otherwise we could also put linking keys together)\n4. Extract session key and component id, forget other columns\n\nThe result is a DataFrame with a component ID for every session. Now let's rename component_id to 'user_handle', which represents our mental concept of the table","dateUpdated":"Jun 11, 2016 4:55:46 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665417_-776498129","id":"20160611-164745_1667552942","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2200","dateFinished":"Jun 11, 2016 4:55:45 PM","dateStarted":"Jun 11, 2016 4:55:45 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Map Back Session Keys</h1>\n<p>Now we have a new graph, where each Vertex has an ID which maps to the original session key or linking key. The value of each Vertex denotes which component the Vertex (and therefore session or linking key) belongs to. By looking up all original session keys and keeping the component ID, we can find all sessions which belong together (those session keys having the same component ID belong to the same user).</p>\n<p>So our plan looks as follows:</p>\n<ol>\n<li>Convert vertices from Graph back into a DataFrame with two columns 'original_idx' and 'component_id'</li>\n<li>Join indexedKeys to the DataFrame on original_idx === index, look up session key</li>\n<li>Keep only session keys (so we can put sessions together. Otherwise we could also put linking keys together)</li>\n<li>Extract session key and component id, forget other columns</li>\n</ol>\n<p>The result is a DataFrame with a component ID for every session. Now let's rename component_id to 'user_handle', which represents our mental concept of the table</p>\n"},"focus":true},{"text":"// 1. Convert Vertices back into a DataFrame\nval resultSchema = StructType(\n    StructField(\"original_idx\", LongType)\n    :: StructField(\"component_idx\", LongType)\n    :: Nil\n)\nval components = sqlContext.createDataFrame(connectedGraph.vertices.map(v => Row(v._1, v._2)), resultSchema)\n\n// 2. Join indexedKeys to the DataFrame components on original_idx === index, look up session key\n\n// 3. Keep only session keys (so we can put sessions together. Otherwise we could also put linking keys together)\n\n// 4. Extract session key and component id, forget other columns\nval sessionMapping = ...","dateUpdated":"Jun 11, 2016 4:53:14 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665417_-776498129","id":"20160611-164745_1900702844","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2201"},{"text":"%md\n## Check Results\nDo the results make sense? Let's have a look. We still have the original user names stored in our original sessions, let's join that data together with the sessionMapping. Let's see if all sessions belonging to a specific user do also have the same component ID.\n\n1. Make tuples (session, name) from original sessions data set distinct\n2. Join the result to sessionMapping\n3. Extract user name and user handle\n4. Sort result by user handle, fetch some entries => Do all entries of a user handle have the same user name?","dateUpdated":"Jun 11, 2016 4:51:22 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665417_-776498129","id":"20160611-164745_1293449983","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2202","dateFinished":"Jun 11, 2016 4:51:22 PM","dateStarted":"Jun 11, 2016 4:51:22 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Check Results</h2>\n<p>Do the results make sense? Let's have a look. We still have the original user names stored in our original sessions, let's join that data together with the sessionMapping. Let's see if all sessions belonging to a specific user do also have the same component ID.</p>\n<ol>\n<li>Make tuples (session, name) from original sessions data set distinct</li>\n<li>Join the result to sessionMapping</li>\n<li>Extract user name and user handle</li>\n<li>Sort result by user handle, fetch some entries => Do all entries of a user handle have the same user name?</li>\n</ol>\n"},"focus":true},{"text":"val combinedUsers = ...\n\ncombinedUsers.toZeppelin(200)","dateUpdated":"Jun 11, 2016 4:52:04 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"session","index":0,"aggr":"sum"}],"values":[{"name":"user_handle","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"session","index":0,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665418_-775343882","id":"20160611-164745_36905700","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2203"},{"text":"%md\n## Check Results by name\n5. Sort result by user name, fetch some entries => Do all entries of a user name have the same user handle?","dateUpdated":"Jun 11, 2016 4:51:58 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665418_-775343882","id":"20160611-164745_1798809564","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2204","dateFinished":"Jun 11, 2016 4:51:58 PM","dateStarted":"Jun 11, 2016 4:51:58 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Check Results by name</h2>\n<ol>\n<li>Sort result by user name, fetch some entries => Do all entries of a user name have the same user handle?</li>\n</ol>\n"},"focus":true},{"text":"combinedUsers.orderBy($\"name\").toZeppelin(200)","dateUpdated":"Jun 11, 2016 4:47:45 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665418_-775343882","id":"20160611-164745_317559682","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2205"},{"text":"%md\n# Cleanup Memory","dateUpdated":"Jun 11, 2016 4:52:01 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665418_-775343882","id":"20160611-164745_728712579","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2206","dateFinished":"Jun 11, 2016 4:52:01 PM","dateStarted":"Jun 11, 2016 4:52:01 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Cleanup Memory</h1>\n"},"focus":true},{"text":"sc.getPersistentRDDs.foreach { entry => entry._2.unpersist() }","dateUpdated":"Jun 11, 2016 4:47:45 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665418_-775343882","id":"20160611-164745_373076412","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2207"},{"dateUpdated":"Jun 11, 2016 4:47:45 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465663665419_-775728631","id":"20160611-164745_1790490891","dateCreated":"Jun 11, 2016 4:47:45 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2208"}],"name":"GraphX Cross Device Linking Exercise","id":"2BPNCNPEK","angularObjects":{"2B44YVSN1":[],"2AJXGMUUJ":[],"2AK8P7CPX":[],"2AM1YV5CU":[],"2AKK3QQXU":[],"2ANGGHHMQ":[]},"config":{"looknfeel":"default"},"info":{}}