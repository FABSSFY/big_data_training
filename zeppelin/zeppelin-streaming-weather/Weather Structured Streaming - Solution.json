{"paragraphs":[{"text":"%md\n# Load Station Data\n\nNow we load the station meta data using traditional SparkSQL DataFrame methods. Since the meta data is stored as a simple CSV, this should be simple. Nevertheless we will explicitly specify a schema, since we do not want to rely on the automatic type inference from Spark.\n","dateUpdated":"2017-02-18T12:16:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077235_572552458","id":"20170109-022408_398981917","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Station Data</h1>\n<p>Now we load the station meta data using traditional SparkSQL DataFrame methods. Since the meta data is stored as a simple CSV, this should be simple. Nevertheless we will explicitly specify a schema, since we do not want to rely on the automatic type inference from Spark.</p>\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T12:16:05+0000","dateFinished":"2017-02-18T12:16:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6409"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.FloatType\nimport org.apache.spark.sql.types.DateType\n\ndef extractFloat = udf((v:String) => if (v != null) v.toFloat else None, FloatType)\n\nval isdSchema = StructType(\n        StructField(\"usaf\", StringType) ::\n        StructField(\"wban\", StringType) ::\n        StructField(\"name\", StringType) ::\n        StructField(\"country\", StringType) ::\n        StructField(\"state\", StringType) ::\n        StructField(\"icao\", StringType) ::\n        StructField(\"latitude\", StringType) ::\n        StructField(\"longitude\", StringType) ::\n        StructField(\"elevation\", StringType) ::\n        StructField(\"date_begin\", DateType) ::\n        StructField(\"date_end\", DateType) ::\n        Nil\n    )\nval isd = sqlContext.read\n    .option(\"header\",\"true\")\n    .option(\"dateFormat\",\"yyyyMMdd\")\n    .schema(isdSchema)\n    .csv(\"s3://dimajix-training/data/weather/isd-history\")\n    .withColumn(\"latitude\", extractFloat($\"latitude\"))\n    .withColumn(\"longitude\", extractFloat($\"longitude\"))\n    .withColumn(\"elevation\", extractFloat($\"elevation\"))\n    \n\nz.show(isd.limit(10))","dateUpdated":"2017-02-18T14:52:36+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077235_572552458","id":"20170109-022420_197453754","result":{"code":"SUCCESS","type":"TABLE","msg":"usaf\twban\tname\tcountry\tstate\ticao\tlatitude\tlongitude\televation\tdate_begin\tdate_end\n007005\t99999\tCWOS 07005\tnull\tnull\tnull\tnull\tnull\tnull\t2012-01-27\t2012-01-27\n007011\t99999\tCWOS 07011\tnull\tnull\tnull\tnull\tnull\tnull\t2011-10-25\t2012-11-29\n007018\t99999\tWXPOD 7018\tnull\tnull\tnull\t0.0\t0.0\t7018.0\t2011-03-09\t2013-07-30\n007025\t99999\tCWOS 07025\tnull\tnull\tnull\tnull\tnull\tnull\t2012-01-27\t2012-01-27\n007026\t99999\tWXPOD 7026\tAF\tnull\tnull\t0.0\t0.0\t7026.0\t2012-07-13\t2014-11-20\n007034\t99999\tCWOS 07034\tnull\tnull\tnull\tnull\tnull\tnull\t2012-10-24\t2012-11-06\n007037\t99999\tCWOS 07037\tnull\tnull\tnull\tnull\tnull\tnull\t2011-12-02\t2012-11-25\n007044\t99999\tCWOS 07044\tnull\tnull\tnull\tnull\tnull\tnull\t2012-01-27\t2012-01-27\n007047\t99999\tCWOS 07047\tnull\tnull\tnull\tnull\tnull\tnull\t2012-06-13\t2012-07-17\n007052\t99999\tCWOS 07052\tnull\tnull\tnull\tnull\tnull\tnull\t2012-11-29\t2012-11-30\n","comment":"","msgTable":[[{"key":"wban","value":"007005"},{"key":"wban","value":"99999"},{"key":"wban","value":"CWOS 07005"},{"key":"wban","value":"null"},{"key":"wban","value":"null"},{"key":"wban","value":"null"},{"key":"wban","value":"null"},{"key":"wban","value":"null"},{"key":"wban","value":"null"},{"key":"wban","value":"2012-01-27"},{"key":"wban","value":"2012-01-27"}],[{"key":"name","value":"007011"},{"key":"name","value":"99999"},{"key":"name","value":"CWOS 07011"},{"key":"name","value":"null"},{"key":"name","value":"null"},{"key":"name","value":"null"},{"key":"name","value":"null"},{"key":"name","value":"null"},{"key":"name","value":"null"},{"key":"name","value":"2011-10-25"},{"key":"name","value":"2012-11-29"}],[{"key":"country","value":"007018"},{"key":"country","value":"99999"},{"key":"country","value":"WXPOD 7018"},{"key":"country","value":"null"},{"key":"country","value":"null"},{"key":"country","value":"null"},{"key":"country","value":"0.0"},{"key":"country","value":"0.0"},{"key":"country","value":"7018.0"},{"key":"country","value":"2011-03-09"},{"key":"country","value":"2013-07-30"}],[{"key":"state","value":"007025"},{"key":"state","value":"99999"},{"key":"state","value":"CWOS 07025"},{"key":"state","value":"null"},{"key":"state","value":"null"},{"key":"state","value":"null"},{"key":"state","value":"null"},{"key":"state","value":"null"},{"key":"state","value":"null"},{"key":"state","value":"2012-01-27"},{"key":"state","value":"2012-01-27"}],[{"key":"icao","value":"007026"},{"key":"icao","value":"99999"},{"key":"icao","value":"WXPOD 7026"},{"key":"icao","value":"AF"},{"key":"icao","value":"null"},{"key":"icao","value":"null"},{"key":"icao","value":"0.0"},{"key":"icao","value":"0.0"},{"key":"icao","value":"7026.0"},{"key":"icao","value":"2012-07-13"},{"key":"icao","value":"2014-11-20"}],[{"key":"latitude","value":"007034"},{"key":"latitude","value":"99999"},{"key":"latitude","value":"CWOS 07034"},{"key":"latitude","value":"null"},{"key":"latitude","value":"null"},{"key":"latitude","value":"null"},{"key":"latitude","value":"null"},{"key":"latitude","value":"null"},{"key":"latitude","value":"null"},{"key":"latitude","value":"2012-10-24"},{"key":"latitude","value":"2012-11-06"}],[{"key":"longitude","value":"007037"},{"key":"longitude","value":"99999"},{"key":"longitude","value":"CWOS 07037"},{"key":"longitude","value":"null"},{"key":"longitude","value":"null"},{"key":"longitude","value":"null"},{"key":"longitude","value":"null"},{"key":"longitude","value":"null"},{"key":"longitude","value":"null"},{"key":"longitude","value":"2011-12-02"},{"key":"longitude","value":"2012-11-25"}],[{"key":"elevation","value":"007044"},{"key":"elevation","value":"99999"},{"key":"elevation","value":"CWOS 07044"},{"key":"elevation","value":"null"},{"key":"elevation","value":"null"},{"key":"elevation","value":"null"},{"key":"elevation","value":"null"},{"key":"elevation","value":"null"},{"key":"elevation","value":"null"},{"key":"elevation","value":"2012-01-27"},{"key":"elevation","value":"2012-01-27"}],[{"key":"date_begin","value":"007047"},{"key":"date_begin","value":"99999"},{"key":"date_begin","value":"CWOS 07047"},{"key":"date_begin","value":"null"},{"key":"date_begin","value":"null"},{"key":"date_begin","value":"null"},{"key":"date_begin","value":"null"},{"key":"date_begin","value":"null"},{"key":"date_begin","value":"null"},{"key":"date_begin","value":"2012-06-13"},{"key":"date_begin","value":"2012-07-17"}],[{"key":"date_end","value":"007052"},{"key":"date_end","value":"99999"},{"key":"date_end","value":"CWOS 07052"},{"key":"date_end","value":"null"},{"key":"date_end","value":"null"},{"key":"date_end","value":"null"},{"key":"date_end","value":"null"},{"key":"date_end","value":"null"},{"key":"date_end","value":"null"},{"key":"date_end","value":"2012-11-29"},{"key":"date_end","value":"2012-11-30"}]],"columnNames":[{"name":"usaf","index":0,"aggr":"sum"},{"name":"wban","index":1,"aggr":"sum"},{"name":"name","index":2,"aggr":"sum"},{"name":"country","index":3,"aggr":"sum"},{"name":"state","index":4,"aggr":"sum"},{"name":"icao","index":5,"aggr":"sum"},{"name":"latitude","index":6,"aggr":"sum"},{"name":"longitude","index":7,"aggr":"sum"},{"name":"elevation","index":8,"aggr":"sum"},{"name":"date_begin","index":9,"aggr":"sum"},{"name":"date_end","index":10,"aggr":"sum"}],"rows":[["007005","99999","CWOS 07005","null","null","null","null","null","null","2012-01-27","2012-01-27"],["007011","99999","CWOS 07011","null","null","null","null","null","null","2011-10-25","2012-11-29"],["007018","99999","WXPOD 7018","null","null","null","0.0","0.0","7018.0","2011-03-09","2013-07-30"],["007025","99999","CWOS 07025","null","null","null","null","null","null","2012-01-27","2012-01-27"],["007026","99999","WXPOD 7026","AF","null","null","0.0","0.0","7026.0","2012-07-13","2014-11-20"],["007034","99999","CWOS 07034","null","null","null","null","null","null","2012-10-24","2012-11-06"],["007037","99999","CWOS 07037","null","null","null","null","null","null","2011-12-02","2012-11-25"],["007044","99999","CWOS 07044","null","null","null","null","null","null","2012-01-27","2012-01-27"],["007047","99999","CWOS 07047","null","null","null","null","null","null","2012-06-13","2012-07-17"],["007052","99999","CWOS 07052","null","null","null","null","null","null","2012-11-29","2012-11-30"]]},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T14:52:36+0000","dateFinished":"2017-02-18T14:53:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6410"},{"text":"isd.printSchema()","dateUpdated":"2017-02-18T14:49:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077236_570628713","id":"20170109-022449_34670710","result":{"code":"SUCCESS","type":"TEXT","msg":"root\n |-- usaf: string (nullable = true)\n |-- wban: string (nullable = true)\n |-- name: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n |-- icao: string (nullable = true)\n |-- latitude: float (nullable = true)\n |-- longitude: float (nullable = true)\n |-- elevation: float (nullable = true)\n |-- date_begin: date (nullable = true)\n |-- date_end: date (nullable = true)\n\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T14:49:15+0000","dateFinished":"2017-02-18T14:52:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6411"},{"text":"%md\n# Spark Structured Streaming","dateUpdated":"2017-02-18T12:14:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077238_571398211","id":"20170109-022507_1035657177","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Spark Structured Streaming</h1>\n"},"dateCreated":"2017-02-18T12:14:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6412"},{"text":"%md\n# 1. Connect to data source\n\nFirst you need to run the netcat program, for example via\n\n    spark-training/utils/pynetcat.py -I1 -B10 -P9977 < weather_sample.txt\n\nThen we connect to the raw data socket as the datasource by using the `DataStreamReader` API via `spark.readStream`. We need to specify the options `host`, `port` and we need to use the format `socket` for connecting to the data source. The socket will stream weather data samples in raw format, i.e. one record per line.","dateUpdated":"2017-02-18T16:18:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077240_569089718","id":"20170109-022532_52613486","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>1. Connect to data source</h1>\n<p>First you need to run the netcat program, for example via</p>\n<pre><code>spark-training/utils/pynetcat.py -I1 -B10 -P9977 &lt; weather_sample.txt\n</code></pre>\n<p>Then we connect to the raw data socket as the datasource by using the <code>DataStreamReader</code> API via <code>spark.readStream</code>. We need to specify the options <code>host</code>, <code>port</code> and we need to use the format <code>socket</code> for connecting to the data source. The socket will stream weather data samples in raw format, i.e. one record per line.</p>\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:28:39+0000","dateFinished":"2017-02-18T15:28:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6413"},{"text":"// Fill in the correct AWS VPC address of your master host\nval master = \"10.200.101.118\"\n\n// Connect to raw text stream socket using the DataStreamReader API via spark.readStream. You need to specify the options `host`, `port` and you need to use the format `socket`\nval lines = spark.readStream\n  .format(\"socket\")\n  .option(\"host\", master)\n  .option(\"port\", 9977)\n  .load()","dateUpdated":"2017-02-18T15:29:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077240_569089718","id":"20170109-022559_819187627","result":{"code":"SUCCESS","type":"TEXT","msg":"\nmaster: String = 10.200.101.118\n\nlines: org.apache.spark.sql.DataFrame = [value: string]\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:18:25+0000","dateFinished":"2017-02-18T15:18:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6414"},{"text":"%md\n## 2. Extract Weather Data\n\nWe need to extract the weather data from the raw string. This can be done using SparkSQL methods. Since this is a rather time-consuming data-fiddling task, you can simply use the code as-is.","dateUpdated":"2017-02-18T15:30:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077240_569089718","id":"20170109-023945_1454623182","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>2. Extract Weather Data</h2>\n<p>We need to extract the weather data from the raw string. This can be done using SparkSQL methods. Since this is a rather time-consuming data-fiddling task, you can simply use the code as-is.</p>\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:30:11+0000","dateFinished":"2017-02-18T15:30:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6415"},{"text":"import org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.types.FloatType\n\nval weather = lines\n    .withColumn(\"date\", lines(\"value\").substr(16,8))\n    .withColumn(\"time\", lines(\"value\").substr(24,4))\n    .withColumn(\"usaf\", lines(\"value\").substr(5,6))\n    .withColumn(\"wban\", lines(\"value\").substr(11,5))\n    .withColumn(\"air_temperature_quality\", lines(\"value\").substr(93,1).cast(IntegerType))\n    .withColumn(\"air_temperature\", lines(\"value\").substr(88,5).cast(FloatType)/10.0)\n    .withColumn(\"wind_speed_quality\", lines(\"value\").substr(70,1).cast(IntegerType))\n    .withColumn(\"wind_speed\", lines(\"value\").substr(66,4).cast(FloatType)/10.0)\n","dateUpdated":"2017-02-18T15:18:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077241_568704969","id":"20170109-022624_1297431457","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.sql.types.IntegerType\n\nimport org.apache.spark.sql.types.FloatType\n\nweather: org.apache.spark.sql.DataFrame = [value: string, date: string ... 7 more fields]\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:18:31+0000","dateFinished":"2017-02-18T15:18:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6416"},{"text":"%md\n## 3. Peek inside Stream\n\nOne nice thing about structured streaming is that it is super easy to peek inside a stream. You need to perform the following steps:\n1. Create a `DataStreamWriter` object using the `writeStream` method of your DataFrame.\n2. Set the format to `console`\n3. Set the output mode to `append`\n4. Specify a `checkPointLocation` on HDFS (ok, this is not trivial, so it is in the code below)\n5. Start the continuous query via `start`","dateUpdated":"2017-02-18T15:32:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077242_569859215","id":"20170109-024025_443642059","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>3. Peek inside Stream</h2>\n<p>One nice thing about structured streaming is that it is super easy to peek inside a stream. You need to perform the following steps:</p>\n<ol>\n<li>Create a <code>DataStreamWriter</code> object using the <code>writeStream</code> method of your DataFrame.</li>\n<li>Set the format to <code>console</code></li>\n<li>Set the output mode to <code>append</code></li>\n<li>Specify a <code>checkPointLocation</code> on HDFS (ok, this is not trivial, so it is in the code below)</li>\n<li>Start the continuous query via <code>start</code></li>\n</ol>\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:32:43+0000","dateFinished":"2017-02-18T15:32:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6417"},{"text":"val query = weather.writeStream\n    .format(\"console\")\n    .outputMode(\"append\")\n    .option(\"checkpointLocation\", \"hdfs://\" + master + \":8020/tmp/zeppelin/checkpoint-print\")\n    .start()","dateUpdated":"2017-02-18T15:20:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077242_569859215","id":"20170109-023319_1896648636","result":{"code":"SUCCESS","type":"TEXT","msg":"\nquery: org.apache.spark.sql.streaming.StreamingQuery = Streaming Query [id = 5f900092-adaf-4eac-af11-85e4cab2428e, runId = 65c270ef-608e-401c-80d7-c24c8b5ff7bf] [state = ACTIVE]\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:18:45+0000","dateFinished":"2017-02-18T15:18:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6418"},{"text":"%md\n### 3.1 Stopping the Query\n\nIn contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the `stop` method on the query object. This makes working with streams much easier.","dateUpdated":"2017-02-18T15:34:04+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487429394358_1093815456","id":"20170218-144954_1651121593","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>3.1 Stopping the Query</h3>\n<p>In contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the <code>stop</code> method on the query object. This makes working with streams much easier.</p>\n"},"dateCreated":"2017-02-18T14:49:54+0000","dateStarted":"2017-02-18T15:34:02+0000","dateFinished":"2017-02-18T15:34:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6419"},{"text":"query.stop()","dateUpdated":"2017-02-18T15:19:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077243_569474467","id":"20170109-023447_927715135","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:19:28+0000","dateFinished":"2017-02-18T15:19:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6420"},{"text":"%md\n## 4. Perform Calculation\n\nAgain we perform a grouped aggregation of some metrics. We are inetersted in the following metrics, grouped by country and year:\n\n* Minimum air temperature\n* Maximum air temperature\n* Minimum wind speed\n* Maximum wind speed\n\nAgain we need to evaulate the \"quality\" fields of the incoming data to decide if the correspong wind speed or air temeprature is valid.","dateUpdated":"2017-02-18T12:14:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077244_567550722","id":"20170109-023458_107574560","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>4. Perform Calculation</h2>\n<p>Again we perform a grouped aggregation of some metrics. We are inetersted in the following metrics, grouped by country and year:</p>\n<ul>\n<li>Minimum air temperature</li>\n<li>Maximum air temperature</li>\n<li>Minimum wind speed</li>\n<li>Maximum wind speed</li>\n</ul>\n<p>Again we need to evaulate the &ldquo;quality&rdquo; fields of the incoming data to decide if the correspong wind speed or air temeprature is valid.</p>\n"},"dateCreated":"2017-02-18T12:14:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6421"},{"text":"val stations = broadcast(isd)\n\nval aggregated_weather = weather\n    // 1. Join weather data with station data on columns 'usaf' and 'wban'\n    .join(stations, weather(\"usaf\") === isd(\"usaf\") && weather(\"wban\") === isd(\"wban\"))\n    // 2. Extract year from date column (first four letters), store it in a new column called  'year'\n    .withColumn(\"year\", weather(\"date\").substr(0,4))\n    // 3. Group by country (from isd) and year (from above)\n    .groupBy(isd(\"country\"), $\"year\")\n    // 4. Perform aggregations of min/max of temperature and wind speed. Again pay attention to quality flags!\n    .agg(\n        min(when(col(\"air_temperature_quality\") === lit(1), col(\"air_temperature\"))).as(\"temp_min\"),\n        max(when(col(\"air_temperature_quality\") === lit(1), col(\"air_temperature\"))).as(\"temp_max\"),\n        min(when(col(\"wind_speed_quality\") === lit(1), col(\"wind_speed\"))).as(\"wind_min\"),\n        max(when(col(\"wind_speed_quality\") === lit(1), col(\"wind_speed\"))).as(\"wind_max\")\n    )","dateUpdated":"2017-02-18T15:36:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077245_567165973","id":"20170109-024107_164598352","result":{"code":"SUCCESS","type":"TEXT","msg":"\nstations: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [usaf: string, wban: string ... 9 more fields]\n\naggregated_weather: org.apache.spark.sql.DataFrame = [country: string, year: string ... 4 more fields]\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:22:14+0000","dateFinished":"2017-02-18T15:22:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6422"},{"text":"%md\n### 4.1 Peek Inside\n\nThis is not what we really want at the end, but let's try to peek inside the infinite streaming table. This is achieved by the same steps as above:\n\n1. Create a DataStreamWriter object using the writeStream method of your DataFrame `aggregated_weather`.\n2. Set the format to `console`\n3. Set the output mode to `complete` (aggregations do not support `append`)\n4. Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)\n5. Start the continuous query via `start`","dateUpdated":"2017-02-18T16:24:28+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487432299201_-1636610691","id":"20170218-153819_1474123022","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>4.1 Peek Inside</h3>\n<p>This is not what we really want at the end, but let's try to peek inside the infinite streaming table. This is achieved by the same steps as above:</p>\n<ol>\n<li>Create a DataStreamWriter object using the writeStream method of your DataFrame <code>aggregated_weather</code>.</li>\n<li>Set the format to <code>console</code></li>\n<li>Set the output mode to <code>complete</code> (aggregations do not support <code>append</code>)</li>\n<li>Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)</li>\n<li>Start the continuous query via <code>start</code></li>\n</ol>\n"},"dateCreated":"2017-02-18T15:38:19+0000","dateStarted":"2017-02-18T16:24:25+0000","dateFinished":"2017-02-18T16:24:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6423","focus":true},{"text":"val consoleQuery = aggregated_weather.writeStream\n    .format(\"console\")\n    .outputMode(\"complete\")\n    .option(\"checkpointLocation\", \"hdfs://\" + master + \":8020/tmp/zeppelin/checkpoint-agg\")\n    .start()","dateUpdated":"2017-02-18T15:23:16+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077246_568320220","id":"20170109-024151_139400986","result":{"code":"SUCCESS","type":"TEXT","msg":"\nconsoleQuery: org.apache.spark.sql.streaming.StreamingQuery = Streaming Query [id = 926eec84-d3cf-4dac-a181-4c6caa4c2a04, runId = 55dc953a-8dcc-4bf6-833f-c397fd271c29] [state = ACTIVE]\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:23:16+0000","dateFinished":"2017-02-18T15:23:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6424"},{"text":"// Stop the Query again\nconsoleQuery.stop()","dateUpdated":"2017-02-18T15:38:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077248_553699762","id":"20170109-024445_883855935","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:23:51+0000","dateFinished":"2017-02-18T15:23:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6425"},{"text":"%md\n## 5. Create live Table\n\nWe can also use a \"memory\" output, which is a queryable live table. In order to do so, we again create a new table, but this time with format `memory` and an explicit query name `aggregated_weather`. Using a `memory` output will create a dynamic table in memory (only `complete` output supported right now), which can be queried using SQL.\n\n1. Create a DataStreamWriter object using the writeStream method of your DataFrame `aggregated_weather`.\n2. Set the format to `memory`\n3. Set the output mode to `complete`\n4. Set the query name to `aggregated_weather`\n5. Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)\n6. Start the continuous query via `start`","dateUpdated":"2017-02-18T16:11:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077248_553699762","id":"20170109-043321_2106212152","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>5. Create live Table</h2>\n<p>We can also use a &ldquo;memory&rdquo; output, which is a queryable live table. In order to do so, we again create a new table, but this time with format <code>memory</code> and an explicit query name <code>aggregated_weather</code>. Using a <code>memory</code> output will create a dynamic table in memory (only <code>complete</code> output supported right now), which can be queried using SQL.</p>\n<ol>\n<li>Create a DataStreamWriter object using the writeStream method of your DataFrame <code>aggregated_weather</code>.</li>\n<li>Set the format to <code>memory</code></li>\n<li>Set the output mode to <code>complete</code></li>\n<li>Set the query name to <code>aggregated_weather</code></li>\n<li>Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)</li>\n<li>Start the continuous query via <code>start</code></li>\n</ol>\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:43:10+0000","dateFinished":"2017-02-18T15:43:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6426"},{"text":"val tableQuery = aggregated_weather.writeStream\n    .outputMode(\"complete\")\n    .format(\"memory\")\n    .queryName(\"aggregated_weather\")\n    .option(\"checkpointLocation\", \"hdfs://\" + master + \":8020/tmp/zeppelin/checkpoint-table\")\n    .start()","dateUpdated":"2017-02-18T15:23:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077249_553315013","id":"20170109-024536_1643682762","result":{"code":"SUCCESS","type":"TEXT","msg":"\ntableQuery: org.apache.spark.sql.streaming.StreamingQuery = Streaming Query aggregated_weather [id = e1ec5c3a-a3c5-4968-a682-6a13f49821e8, runId = c4a38782-f9e4-4962-b90f-149744150bde] [state = ACTIVE]\n"},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:23:54+0000","dateFinished":"2017-02-18T15:23:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6427"},{"text":"%sql\nselect * from aggregated_weather","dateUpdated":"2017-02-18T15:24:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"country","index":0,"aggr":"sum"}],"values":[{"name":"year","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"country","index":0,"aggr":"sum"},"yAxis":{"name":"year","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077249_553315013","id":"20170109-043226_2033527472","result":{"code":"SUCCESS","type":"TABLE","msg":"country\tyear\ttemp_min\ttemp_max\twind_min\twind_max\nUK\t2004\t12.2\t12.2\t6.2\t6.2\nPO\t2004\t9.8\t14.0\t1.0\t1.5\nGK\t2004\t11.5\t11.5\t9.8\t9.8\nBE\t2004\t8.6\t11.0\t6.2\t8.0\nUS\t2004\t-12.0\t15.0\t0.0\t5.1\nNO\t2004\t-4.0\t1.0\t2.6\t8.2\nSC\t2004\t26.2\t27.0\t5.7\t5.7\nFI\t2004\t-19.0\t-19.0\t0.5\t0.5\nSW\t2004\t-16.2\t-16.2\t2.0\t2.0\nNL\t2004\t11.0\t12.0\t6.2\t19.0\nCA\t2004\tnull\tnull\tnull\tnull\nFR\t2004\t11.0\t11.0\t3.1\t3.1\nAM\t2004\t11.0\t11.0\t2.0\t2.0\n","comment":"","msgTable":[[{"key":"year","value":"UK"},{"key":"year","value":"2004"},{"key":"year","value":"12.2"},{"key":"year","value":"12.2"},{"key":"year","value":"6.2"},{"key":"year","value":"6.2"}],[{"key":"temp_min","value":"PO"},{"key":"temp_min","value":"2004"},{"key":"temp_min","value":"9.8"},{"key":"temp_min","value":"14.0"},{"key":"temp_min","value":"1.0"},{"key":"temp_min","value":"1.5"}],[{"key":"temp_max","value":"GK"},{"key":"temp_max","value":"2004"},{"key":"temp_max","value":"11.5"},{"key":"temp_max","value":"11.5"},{"key":"temp_max","value":"9.8"},{"key":"temp_max","value":"9.8"}],[{"key":"wind_min","value":"BE"},{"key":"wind_min","value":"2004"},{"key":"wind_min","value":"8.6"},{"key":"wind_min","value":"11.0"},{"key":"wind_min","value":"6.2"},{"key":"wind_min","value":"8.0"}],[{"key":"wind_max","value":"US"},{"key":"wind_max","value":"2004"},{"key":"wind_max","value":"-12.0"},{"key":"wind_max","value":"15.0"},{"key":"wind_max","value":"0.0"},{"key":"wind_max","value":"5.1"}],[{"value":"NO"},{"value":"2004"},{"value":"-4.0"},{"value":"1.0"},{"value":"2.6"},{"value":"8.2"}],[{"value":"SC"},{"value":"2004"},{"value":"26.2"},{"value":"27.0"},{"value":"5.7"},{"value":"5.7"}],[{"value":"FI"},{"value":"2004"},{"value":"-19.0"},{"value":"-19.0"},{"value":"0.5"},{"value":"0.5"}],[{"value":"SW"},{"value":"2004"},{"value":"-16.2"},{"value":"-16.2"},{"value":"2.0"},{"value":"2.0"}],[{"value":"NL"},{"value":"2004"},{"value":"11.0"},{"value":"12.0"},{"value":"6.2"},{"value":"19.0"}],[{"value":"CA"},{"value":"2004"},{"value":"null"},{"value":"null"},{"value":"null"},{"value":"null"}],[{"value":"FR"},{"value":"2004"},{"value":"11.0"},{"value":"11.0"},{"value":"3.1"},{"value":"3.1"}],[{"value":"AM"},{"value":"2004"},{"value":"11.0"},{"value":"11.0"},{"value":"2.0"},{"value":"2.0"}]],"columnNames":[{"name":"country","index":0,"aggr":"sum"},{"name":"year","index":1,"aggr":"sum"},{"name":"temp_min","index":2,"aggr":"sum"},{"name":"temp_max","index":3,"aggr":"sum"},{"name":"wind_min","index":4,"aggr":"sum"},{"name":"wind_max","index":5,"aggr":"sum"}],"rows":[["UK","2004","12.2","12.2","6.2","6.2"],["PO","2004","9.8","14.0","1.0","1.5"],["GK","2004","11.5","11.5","9.8","9.8"],["BE","2004","8.6","11.0","6.2","8.0"],["US","2004","-12.0","15.0","0.0","5.1"],["NO","2004","-4.0","1.0","2.6","8.2"],["SC","2004","26.2","27.0","5.7","5.7"],["FI","2004","-19.0","-19.0","0.5","0.5"],["SW","2004","-16.2","-16.2","2.0","2.0"],["NL","2004","11.0","12.0","6.2","19.0"],["CA","2004","null","null","null","null"],["FR","2004","11.0","11.0","3.1","3.1"],["AM","2004","11.0","11.0","2.0","2.0"]]},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:24:09+0000","dateFinished":"2017-02-18T15:24:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6428"},{"text":"tableQuery.stop()","dateUpdated":"2017-02-18T15:24:17+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077249_553315013","id":"20170109-043235_704854873","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-18T12:14:37+0000","dateStarted":"2017-02-18T15:24:17+0000","dateFinished":"2017-02-18T15:24:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6429"},{"text":"","dateUpdated":"2017-02-18T12:14:37+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487420077250_554469259","id":"20170109-043303_576621974","dateCreated":"2017-02-18T12:14:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6430"}],"name":"Weather Structured Streaming - Solution","id":"2CC25ATBJ","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}