{"paragraphs":[{"text":"%md\n# Load Data\n\nFirst we load data from HDFS. It is stored as a trivial CSV file with three columns\n\n    product name\n    review text\n    rating (1 - 5)\n\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470066_-1515886082","id":"20170129-045139_1503434840","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Data</h1>\n<p>First we load data from HDFS. It is stored as a trivial CSV file with three columns</p>\n<pre><code>product name\nreview text\nrating (1 - 5)\n</code></pre>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2841"},{"text":"//val basedir = \"file:///home/cloudera/Training/data/\"\nval basedir = \"s3://dimajix-training/data/\"","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470067_-1516270831","id":"20170129-091247_907732707","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2842"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.functions.col\n\n\nval schema = StructType(\n        StructField(\"name\", StringType) ::\n        StructField(\"review\", StringType) ::\n        StructField(\"rating\", StringType) ::\n        Nil\n    )\nval data = sqlContext.read\n    .schema(schema)\n    .csv(basedir + \"amazon_baby\")\n    .withColumn(\"rating\", col(\"rating\").cast(IntegerType))\n    .filter(col(\"rating\").isNotNull)\nz.show(data.limit(10))","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470068_-1518194576","id":"20170129-045155_986374275","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2843"},{"text":"%md\n# Split Train Data / Test Data\n\nNow let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. The sampling can be done via the DataFrame method `randomSplit`.\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470068_-1518194576","id":"20170129-045205_1227820739","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Split Train Data / Test Data</h1>\n<p>Now let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. The sampling can be done via the DataFrame method <code>randomSplit</code>.</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2844"},{"text":"val Array(trainingData, validationData) = // YOUR CODE HERE\n\nprintln(s\"trainingData: ${trainingData.count()}\")\nprintln(s\"validationData: ${validationData.count()}\")","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470069_-1518579325","id":"20170129-045214_331567309","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2845"},{"text":"%md\n# Implement Transformer\n\nWe need a custom Transformer to build the pipeline. The transformer should remove all punctuations from a given column containing text.\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470069_-1518579325","id":"20170129-045225_1360438852","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Implement Transformer</h1>\n<p>We need a custom Transformer to build the pipeline. The transformer should remove all punctuations from a given column containing text.</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2846"},{"text":"import org.apache.spark.ml.Transformer\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions.col\n\n\nclass PunctuationCleanupTransformer(override val uid: String) extends org.apache.spark.ml.Transformer {\n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val inputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"inputCol\", \"input column name\")\n    setDefault(inputCol, \"input\")\n    def getInputCol: String = $(inputCol)\n    def setInputCol(value:String): this.type = set(inputCol, value)\n\n    val outputCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"outputCol\", \"output column name\")\n    setDefault(outputCol, \"output\")\n    def getOutputCol: String = $(outputCol)\n    def setOutputCol(value:String): this.type = set(outputCol, value)\n    \n    override def transform(dataset:org.apache.spark.sql.Dataset[_]) = {\n        val removePunctuation = org.apache.spark.sql.functions.udf { text:String => if (text != null) text.replaceAll(\"\\\\p{Punct}\", \" \") else \"\" }\n        dataset.withColumn(getOutputCol, removePunctuation(dataset(getInputCol)))\n    }\n    \n    override def transformSchema(schema:org.apache.spark.sql.types.StructType) = {\n        val outputColName = $(outputCol)\n        val outCol = org.apache.spark.sql.types.StructField(outputColName, org.apache.spark.sql.types.StringType)\n        \n        if (schema.fieldNames.contains(outputColName)) {\n          throw new IllegalArgumentException(s\"Output column $outputColName already exists.\")\n        }\n        org.apache.spark.sql.types.StructType(schema.fields :+ outCol)\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): PunctuationCleanupTransformer = defaultCopy(extra)\n}\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470070_-1517425078","id":"20170129-045233_1450874507","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2847"},{"text":"%md\n## Test Transformer\n\nLets create an instance of the Transformer and test it. This involves the following steps\n1. Create an instance of the class PunctuationCleanupTransformer\n2. Configure input and output column names in transfomer\n3. Transform the data and store the result in a DataFrame `clean_data`\n \nOptionally you can also display the first couple of entries using `z.show()`.\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470071_-1517809827","id":"20170129-045246_1519878179","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Test Transformer</h2>\n<p>Lets create an instance of the Transformer and test it. This involves the following steps</p>\n<ol>\n<li>Create an instance of the class PunctuationCleanupTransformer</li>\n<li>Configure input and output column names in transfomer</li>\n<li>Transform the data and store the result in a DataFrame <code>clean_data</code></li>\n</ol>\n<p>Optionally you can also display the first couple of entries using <code>z.show()</code>.</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2848"},{"text":"val cleaner = ... // YOUR CODE HERE\nval clean_data = ... // YOUR CODE HERE\n\nz.show(clean_data.limit(6))","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470071_-1517809827","id":"20170129-045257_1112043389","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2849"},{"text":"%md\n# Create ML Pipeline\n\nNow we have all components for creating an initial ML Pipeline. Remember that we have been using the following components before\n\n* `Tokenizer` - for splitting reviews into words\n* `StopWordRemover` - for removing stop words\n* `NGram` - for creating n-grams (tuples of consecutive words)\n* `CountVectorizer` - for creating bag-of-word features from the words\n* `IDF` - for creating a TF-IDF model from the raw counts\n* `LogisticRegression` - for creating the real model\n\nYou also need to perform the following transforms:\n* The incoming rating (1-5) needs to be mapped to a sentiment (0 or 1) and you need to drop reviews with a rating of 3. This can be done using one ore more `SQLTransformer` instances. You may want to look that up in the Spark documentation (org.apache.spark.ml.feature.SQLTransformer).\n* The Punctuations need to be removed. This can be done using the `PunctuationRemovalTransfomer`.\n\nThe logical order of feature transformation should be\n1. Remove punctuations\n2. Create sentiment\n3. Tokenize text data\n4. Remove stop words\n5. Create Bi-Grams (N-Grams with two words each)\n6. Count words per review\n7. Create TF-IDF model\n","dateUpdated":"2017-02-26T10:45:25+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470071_-1517809827","id":"20170129-045307_120063111","result":{"code":"SUCCESS","type":"HTML","msg":"<div class=\"markdown-body\">\n<h1>Create ML Pipeline</h1>\n<p>Now we have all components for creating an initial ML Pipeline. Remember that we have been using the following components before</p>\n<ul>\n  <li><code>Tokenizer</code> - for splitting reviews into words</li>\n  <li><code>StopWordRemover</code> - for removing stop words</li>\n  <li><code>NGram</code> - for creating n-grams (tuples of consecutive words)</li>\n  <li><code>CountVectorizer</code> - for creating bag-of-word features from the words</li>\n  <li><code>IDF</code> - for creating a TF-IDF model from the raw counts</li>\n  <li><code>LogisticRegression</code> - for creating the real model</li>\n</ul>\n<p>You also need to perform the following transforms:<br/>* The incoming rating (1-5) needs to be mapped to a sentiment (0 or 1) and you need to drop reviews with a rating of 3. This can be done using one ore more <code>SQLTransformer</code> instances. You may want to look that up in the Spark documentation (org.apache.spark.ml.feature.SQLTransformer).<br/>* The Punctuations need to be removed. This can be done using the <code>PunctuationRemovalTransfomer</code>.</p>\n<p>The logical order of feature transformation should be<br/>1. Remove punctuations<br/>2. Create sentiment<br/>3. Tokenize text data<br/>4. Remove stop words<br/>5. Create Bi-Grams (N-Grams with two words each)<br/>6. Count words per review<br/>7. Create TF-IDF model</p>\n</div>"},"dateCreated":"2017-02-26T10:37:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2850","user":"anonymous","dateFinished":"2017-02-26T10:45:27+0000","dateStarted":"2017-02-26T10:45:25+0000"},{"text":"import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.classification._\n\n// First let's define a list of stop words\nval stopWords = Array(\"the\",\"a\",\"and\",\"or\", \"it\", \"this\", \"of\", \"an\", \"as\", \"in\", \"on\", \"is\", \"are\", \"to\", \"was\", \"for\", \"then\", \"i\", \"my\", \"that\", \"with\", \"we\", \"have\", \"so\", \"you\", \"s\", \"\")\n\n// Now let's define an array of pipeline stages. Each stage (except the last one) is a transformer, and the last one if the LogisticRegression.\nval stages = Array(\n    new PunctuationCleanupTransformer()\n        // YOUR CODE HERE\n        ,\n    new SQLTransformer()\n        // YOUR CODE HERE\n        ,\n    new Tokenizer()\n        // YOUR CODE HERE\n        ,\n    new StopWordsRemover()\n        // YOUR CODE HERE\n        ,\n    new NGram()\n        // YOUR CODE HERE\n        ,\n    new CountVectorizer()\n        // YOUR CODE HERE\n        ,\n    new IDF()\n        // YOUR CODE HERE\n        ,\n    new LogisticRegression()\n        // YOUR CODE HERE\n)\nval pipe = new Pipeline().setStages(stages)","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470072_-1519733571","id":"20170129-045329_2143290843","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2851"},{"text":"%md\n## Fit Pipeline Model\n\nUsing training data, we create a PipelineModel by fitting the Pipeline to the data.\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470073_-1520118320","id":"20170129-045341_907324707","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Fit Pipeline Model</h2>\n<p>Using training data, we create a PipelineModel by fitting the Pipeline to the data.</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2852"},{"text":"val model = ... // YOUR CODE HERE","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470073_-1520118320","id":"20170129-045351_1240420989","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2853"},{"text":"%md\n## Predict Data\n\nLet us do some predictions of the test data using the model.\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470074_-1518964074","id":"20170129-045405_2130383484","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Predict Data</h2>\n<p>Let us do some predictions of the test data using the model.</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2854"},{"text":"val pred = ... // YOUR CODE HERE\n\nz.show(pred.limit(10))","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470074_-1518964074","id":"20170129-045410_1701329257","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2855"},{"text":"%md\n# Model Evaluation\n\nAs in the original exercise, we want to use a custom metric for assessing the performance.\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470075_-1519348822","id":"20170129-045421_315570748","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Model Evaluation</h1>\n<p>As in the original exercise, we want to use a custom metric for assessing the performance.</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2856"},{"text":"import org.apache.spark.sql.functions.col\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.ml.param.Param\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.evaluation.Evaluator\n\nclass AccuracyClassificationEvaluator(override val uid: String) extends org.apache.spark.ml.evaluation.Evaluator {\n    \n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val labelCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"labelCol\", \"label column name\")\n    setDefault(labelCol, \"label\")\n    def getLabelCol: String = $(labelCol)\n    def setLabelCol(value:String): this.type = set(labelCol, value)\n\n    val predictionCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"predictionCol\", \"prediction column name\")\n    setDefault(predictionCol, \"prediction\")\n    def getPredictionCol: String = $(predictionCol)\n    def setPredictionCol(value:String): this.type = set(predictionCol, value)\n\n    override def evaluate(dataset: org.apache.spark.sql.Dataset[_]) : Double = {\n        val num_total = dataset.count()\n        val num_correct = dataset.filter(dataset(getLabelCol) === dataset(getPredictionCol)).count()\n        num_correct.toDouble / num_total\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): AccuracyClassificationEvaluator = null // defaultCopy(extra)\n\n}","dateUpdated":"2017-02-26T10:37:50+0000","config":{"colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470075_-1519348822","id":"20170129-045431_175924070","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2857"},{"text":"%md\n## Assess Performance\n\nWith the evaluator we can assess the performance of the prediction and easily compare it to a simple model which always predicts 'positive'.\n\nFor evaluating a model, the following steps have to be performed\n1. Create instance of `AccuracyClassificationEvaluator`\n2. Set names of label column and prediction in evaluator\n3. Evaluate trained model\n4. Evaluate dummy model which always predicts \"positive\" as a baseline\n\nHow can we create a model that always predicts 'positive'? Actually we don't need a model itself, since evaluation is done by looking at the predicted values. So it is enough to create a DataFrame with a prediction column with constant value '1'.\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470076_-1521272567","id":"20170129-045442_862289758","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Assess Performance</h2>\n<p>With the evaluator we can assess the performance of the prediction and easily compare it to a simple model which always predicts 'positive'.</p>\n<p>For evaluating a model, the following steps have to be performed</p>\n<ol>\n<li>Create instance of <code>AccuracyClassificationEvaluator</code></li>\n<li>Set names of label column and prediction in evaluator</li>\n<li>Evaluate trained model</li>\n<li>Evaluate dummy model which always predicts &ldquo;positive&rdquo; as a baseline</li>\n</ol>\n<p>How can we create a model that always predicts 'positive'? Actually we don't need a model itself, since evaluation is done by looking at the predicted values. So it is enough to create a DataFrame with a prediction column with constant value '1'.</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2858"},{"text":"val always_positive = pred.withColumn(\"prediction\",lit(1.0))\n\nval evaluator = ... // YOUR CODE HERE\n\nval accuracy = ... // YOUR CODE HERE\nval baseline = ... // YOUR CODE HERE\nprintln(s\"Model Accuracy = ${accuracy}\")\nprintln(s\"Baseline Accuracy = ${baseline}\")","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470076_-1521272567","id":"20170129-045451_732519493","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2859"},{"text":"%md\n# Hyper Parameter Tuning\n\nAgain we want to tune some hyper parameters, but this time inside a pipeline. The methodology is the same as before, we can directly include the CrossValidator into the pipeline. But step by step...\n\nFirst let us have a look at all paremeters of a LogisticRegression.\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470077_-1521657316","id":"20170129-045504_1499849390","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Hyper Parameter Tuning</h1>\n<p>Again we want to tune some hyper parameters, but this time inside a pipeline. The methodology is the same as before, we can directly include the CrossValidator into the pipeline. But step by step&hellip;</p>\n<p>First let us have a look at all paremeters of a LogisticRegression.</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2860"},{"text":"println(new LogisticRegression().explainParams())","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470078_-1520503069","id":"20170129-045513_83457101","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2861"},{"text":"%md\n## Create ParamGrid\n\nNow we create a param grid that should be used for using different sets of parameters. We want to tweak two parameters again:\n\n* regParam should take values in [0.0, 0.0001, 0.01, 1.0, 100.0]\n* maxIter should take values in [10, 100])\n\nIn order to create this grid, we first need to create an instance of a LogisticRegression, so we can access its parameters.\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470078_-1520503069","id":"20170129-045529_389742601","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Create ParamGrid</h2>\n<p>Now we create a param grid that should be used for using different sets of parameters. We want to tweak two parameters again:</p>\n<ul>\n<li>regParam should take values in [0.0, 0.0001, 0.01, 1.0, 100.0]</li>\n<li>maxIter should take values in [10, 100])</li>\n</ul>\n<p>In order to create this grid, we first need to create an instance of a LogisticRegression, so we can access its parameters.</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2862"},{"text":"import org.apache.spark.ml.tuning.ParamGridBuilder\n\nval lr = new LogisticRegression()\n    .setFeaturesCol(\"features\")\n    .setLabelCol(\"sentiment\")\nval param_grid = new ParamGridBuilder()\n    .addGrid(lr.regParam, Array(0.0, 0.0001, 0.01, 1.0, 100.0))\n    .addGrid(lr.maxIter, Array(10, 100))\n    .build()","dateUpdated":"2017-02-26T10:37:50+0000","config":{"colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470079_-1520887818","id":"20170129-045537_1963157387","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2863"},{"text":"%md\n## Create Pipeline\n\nNow we can create a pipeline using a CrossValidator instead of directly using a LogisticRegression. This means the configuration of the Pipeline should match the old one except that a CrossValidator is inserted instead of the LogisticRegression. The CrossValidator works as a wrapper of the regression algorithm.\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470080_-1436627809","id":"20170129-045559_83967585","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Create Pipeline</h2>\n<p>Now we can create a pipeline using a CrossValidator instead of directly using a LogisticRegression. This means the configuration of the Pipeline should match the old one except that a CrossValidator is inserted instead of the LogisticRegression. The CrossValidator works as a wrapper of the regression algorithm.</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2864"},{"text":"import org.apache.spark.ml.tuning.CrossValidator\n\nval stopWords = Array(\"the\",\"a\",\"and\",\"or\", \"it\", \"this\", \"of\", \"an\", \"as\", \"in\", \"on\", \"is\", \"are\", \"to\", \"was\", \"for\", \"then\", \"i\", \"my\", \"that\", \"with\", \"we\", \"have\", \"so\", \"you\", \"s\", \"\")\n\nval stages = Array(\n    new PunctuationCleanupTransformer()\n        // YOUR CODE HERE\n        ...\n    // MORE TRANSFORMS HERE\n    ...\n    new CrossValidator()\n        .setEstimator(new LogisticRegression()\n            .setFeaturesCol(\"features\")\n            .setLabelCol(\"sentiment\")\n        )\n        .setEstimatorParamMaps(param_grid)\n        .setEvaluator(new AccuracyClassificationEvaluator()\n            .setLabelCol(\"sentiment\")\n        )\n        .setNumFolds(3)\n)\n\nval pipe = new Pipeline().setStages(stages)\nval model = pipe.fit(trainingData)","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470080_-1436627809","id":"20170129-045609_1796804187","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2865"},{"text":"%md\n## Assess model performance\nAgain we want to evaluate the model that was trained using the pipeline and compare it the same simple baseline model which always predicts 'positive'","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470081_-1437012558","id":"20170131-103049_1557536721","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Assess model performance</h2>\n<p>Again we want to evaluate the model that was trained using the pipeline and compare it the same simple baseline model which always predicts 'positive'</p>\n"},"dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2866"},{"text":"// Predict sentiment for test data\nval pred = ... // YOUR CODE HERE\n\nval accuracy = ... // YOUR CODE HERE\nval baseline = ... // YOUR CODE HERE\nprintln(s\"Model Accuracy = ${accuracy}\")\nprintln(s\"Baseline Accuracy = ${baseline}\")\n","dateUpdated":"2017-02-26T10:37:50+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470082_-1435858311","id":"20170129-045619_1930087860","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2867"},{"text":"","dateUpdated":"2017-02-26T10:37:50+0000","config":{"colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1488105470082_-1435858311","id":"20170129-101018_1552288438","dateCreated":"2017-02-26T10:37:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2868"}],"name":"Amazon Baby Products Pipeline Exercise","id":"2C9GG6WBM","angularObjects":{"2C9DA5ABT:shared_process":[],"2CB4W1XS9:shared_process":[],"2CAV382KZ:shared_process":[],"2CCPAMJ23:shared_process":[],"2C9H3K6VR:shared_process":[],"2CAEG9DUN:shared_process":[],"2CCMDDDKG:shared_process":[],"2CCAMQ586:shared_process":[],"2CAF2WTQ3:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}