{"paragraphs":[{"text":"%md\n# Load Data\n\nFirst we load data from HDFS or S3. It is stored as a trivial CSV file with three columns\n\n* product name\n* review text\n* rating (1 - 5)\n\nWe will explicitly specify a Schema, because the CSV might not contain a valid header. Moreover this keeps us in detailed control over the column types and saves us from one Spark run trying to guess the types. ","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875489_-324204598","id":"20170129-041356_575741458","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Data</h1>\n<p>First we load data from HDFS or S3. It is stored as a trivial CSV file with three columns</p>\n<ul>\n<li>product name</li>\n<li>review text</li>\n<li>rating (1 - 5)</li>\n</ul>\n<p>We will explicitly specify a Schema, because the CSV might not contain a valid header. Moreover this keeps us in detailed control over the column types and saves us from one Spark run trying to guess the types.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:255"},{"text":"val basedir = \"file:///home/cloudera/Training/data/\"\n//val basedir = \"s3://dimajix-training/data/\"","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875489_-324204598","id":"20170129-045843_673583787","result":{"code":"SUCCESS","type":"TEXT","msg":"\nbasedir: String = file:///home/cloudera/Training/data/\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:256"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.functions.col\n\n// Explicitly build the schema as we expect it in the CSV file. Note that we also assume \"rating\" to be string type. This saves\n// us from errors due to wrong entries. Casting a String to something else later only produces SQL NULL values, but no errors.\nval schema = StructType(\n        StructField(\"name\", StringType) ::\n        StructField(\"review\", StringType) ::\n        StructField(\"rating\", StringType) ::\n        Nil\n    )\n// Read in CSV data and cast rating column to integer    \nval csvdata = sqlContext.read\n    .schema(schema)\n    .csv(basedir + \"amazon_baby\")\n    .withColumn(\"rating\", col(\"rating\").cast(IntegerType))\n    .filter(col(\"rating\").isNotNull)\nz.show(csvdata.limit(10))","dateUpdated":"2017-02-01T10:10:43-0800","config":{"enabled":true,"graph":{"mode":"table","height":230.36666870117188,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"review","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"review","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875490_-323050351","id":"20170129-041420_2034427172","result":{"code":"SUCCESS","type":"TABLE","msg":"name\treview\trating\nPlanetwise Flannel Wipes\tThese flannel wipes are OK, but in my opinion not worth keeping.  I also ordered someImse Vimse Cloth Wipes-Ocean Blue-12 countwhich are larger, had a nicer, softer texture and just seemed higher quality.  I use cloth wipes for hands and faces and have been usingThirsties 6 Pack Fab Wipes, Boyfor about 8 months now and need to replace them because they are starting to get rough and have had stink issues for a while that stripping no longer handles.\t3\nPlanetwise Wipe Pouch\tit came early and was not disappointed. i love planet wise bags and now my wipe holder. it keps my osocozy wipes moist and does not leak. highly recommend it.\t5\nAnnas Dream Full Quilt with 2 Shams\tVery soft and comfortable and warmer than it looks...fit the full size bed perfectly...would recommend to anyone looking for this type of quilt\t5\nStop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book\tThis is a product well worth the purchase.  I have not found anything else like this, and it is a positive, ingenious approach to losing the binky.  What I love most about this product is how much ownership my daughter has in getting rid of the binky.  She is so proud of herself, and loves her little fairy.  I love the artwork, the chart in the back, and the clever approach of this tool.\t5\nStop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book\tAll of my kids have cried non-stop when I tried to ween them off their pacifier, until I found Thumbuddy To Love's Binky Fairy Puppet.  It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from it.This is a must buy book, and a great gift for expecting parents!!  You will save them soo many headaches.Thanks for this book!  You all rock!!\t5\nStop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book\tWhen the Binky Fairy came to our house, we didn't have any special gift and book to help explain to her about how important it is to stop using a pacifier. This book does a great job to help prepare your child for the loss of their favorite item. The doll is adorable and we made lots of cute movies with the Binky Fairy telling our daughter about what happens when the Binky Fairy comes. I would highly recommend this product for any parent trying to break the pacifier or thumb sucking habit.\t5\nA Tale of Baby's Days with Peter Rabbit\tLovely book, it's bound tightly so you may not be able to add alot of photos/cards aside from the designated spaces in the book. Shop around before you purchase, as it is currently listed at Barnes & Noble for 29.95!\t4\nBaby Tracker&reg; - Daily Childcare Journal, Schedule Log\tPerfect for new parents. We were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. Made life easier when the doctor would ask questions about habits because we had it all right there!\t5\nBaby Tracker&reg; - Daily Childcare Journal, Schedule Log\tA friend of mine pinned this product on Pinterest so I decided to give it a whirl! It is fantastic! If you are a new parent, this will help you keep track of feedings, diaper changes and the like!\t5\nBaby Tracker&reg; - Daily Childcare Journal, Schedule Log\tThis has been an easy way for my nanny to record all the key events that happen with my baby when I'm not at home.  Would highly recommend it to someone who wants to stay informed of what your baby is up to while you're not home.The only reason this isn't a 5 is because I think there could have been some more standarad pre-printed options.I plan on ordering another one when we run out of pages in this journal.\t4\n","comment":"","msgTable":[[{"key":"review","value":"Planetwise Flannel Wipes"},{"key":"review","value":"These flannel wipes are OK, but in my opinion not worth keeping.  I also ordered someImse Vimse Cloth Wipes-Ocean Blue-12 countwhich are larger, had a nicer, softer texture and just seemed higher quality.  I use cloth wipes for hands and faces and have been usingThirsties 6 Pack Fab Wipes, Boyfor about 8 months now and need to replace them because they are starting to get rough and have had stink issues for a while that stripping no longer handles."},{"key":"review","value":"3"}],[{"key":"rating","value":"Planetwise Wipe Pouch"},{"key":"rating","value":"it came early and was not disappointed. i love planet wise bags and now my wipe holder. it keps my osocozy wipes moist and does not leak. highly recommend it."},{"key":"rating","value":"5"}],[{"value":"Annas Dream Full Quilt with 2 Shams"},{"value":"Very soft and comfortable and warmer than it looks...fit the full size bed perfectly...would recommend to anyone looking for this type of quilt"},{"value":"5"}],[{"value":"Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book"},{"value":"This is a product well worth the purchase.  I have not found anything else like this, and it is a positive, ingenious approach to losing the binky.  What I love most about this product is how much ownership my daughter has in getting rid of the binky.  She is so proud of herself, and loves her little fairy.  I love the artwork, the chart in the back, and the clever approach of this tool."},{"value":"5"}],[{"value":"Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book"},{"value":"All of my kids have cried non-stop when I tried to ween them off their pacifier, until I found Thumbuddy To Love's Binky Fairy Puppet.  It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from it.This is a must buy book, and a great gift for expecting parents!!  You will save them soo many headaches.Thanks for this book!  You all rock!!"},{"value":"5"}],[{"value":"Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book"},{"value":"When the Binky Fairy came to our house, we didn't have any special gift and book to help explain to her about how important it is to stop using a pacifier. This book does a great job to help prepare your child for the loss of their favorite item. The doll is adorable and we made lots of cute movies with the Binky Fairy telling our daughter about what happens when the Binky Fairy comes. I would highly recommend this product for any parent trying to break the pacifier or thumb sucking habit."},{"value":"5"}],[{"value":"A Tale of Baby's Days with Peter Rabbit"},{"value":"Lovely book, it's bound tightly so you may not be able to add alot of photos/cards aside from the designated spaces in the book. Shop around before you purchase, as it is currently listed at Barnes & Noble for 29.95!"},{"value":"4"}],[{"value":"Baby Tracker&reg; - Daily Childcare Journal, Schedule Log"},{"value":"Perfect for new parents. We were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. Made life easier when the doctor would ask questions about habits because we had it all right there!"},{"value":"5"}],[{"value":"Baby Tracker&reg; - Daily Childcare Journal, Schedule Log"},{"value":"A friend of mine pinned this product on Pinterest so I decided to give it a whirl! It is fantastic! If you are a new parent, this will help you keep track of feedings, diaper changes and the like!"},{"value":"5"}],[{"value":"Baby Tracker&reg; - Daily Childcare Journal, Schedule Log"},{"value":"This has been an easy way for my nanny to record all the key events that happen with my baby when I'm not at home.  Would highly recommend it to someone who wants to stay informed of what your baby is up to while you're not home.The only reason this isn't a 5 is because I think there could have been some more standarad pre-printed options.I plan on ordering another one when we run out of pages in this journal."},{"value":"4"}]],"columnNames":[{"name":"name","index":0,"aggr":"sum"},{"name":"review","index":1,"aggr":"sum"},{"name":"rating","index":2,"aggr":"sum"}],"rows":[["Planetwise Flannel Wipes","These flannel wipes are OK, but in my opinion not worth keeping.  I also ordered someImse Vimse Cloth Wipes-Ocean Blue-12 countwhich are larger, had a nicer, softer texture and just seemed higher quality.  I use cloth wipes for hands and faces and have been usingThirsties 6 Pack Fab Wipes, Boyfor about 8 months now and need to replace them because they are starting to get rough and have had stink issues for a while that stripping no longer handles.","3"],["Planetwise Wipe Pouch","it came early and was not disappointed. i love planet wise bags and now my wipe holder. it keps my osocozy wipes moist and does not leak. highly recommend it.","5"],["Annas Dream Full Quilt with 2 Shams","Very soft and comfortable and warmer than it looks...fit the full size bed perfectly...would recommend to anyone looking for this type of quilt","5"],["Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book","This is a product well worth the purchase.  I have not found anything else like this, and it is a positive, ingenious approach to losing the binky.  What I love most about this product is how much ownership my daughter has in getting rid of the binky.  She is so proud of herself, and loves her little fairy.  I love the artwork, the chart in the back, and the clever approach of this tool.","5"],["Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book","All of my kids have cried non-stop when I tried to ween them off their pacifier, until I found Thumbuddy To Love's Binky Fairy Puppet.  It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from it.This is a must buy book, and a great gift for expecting parents!!  You will save them soo many headaches.Thanks for this book!  You all rock!!","5"],["Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book","When the Binky Fairy came to our house, we didn't have any special gift and book to help explain to her about how important it is to stop using a pacifier. This book does a great job to help prepare your child for the loss of their favorite item. The doll is adorable and we made lots of cute movies with the Binky Fairy telling our daughter about what happens when the Binky Fairy comes. I would highly recommend this product for any parent trying to break the pacifier or thumb sucking habit.","5"],["A Tale of Baby's Days with Peter Rabbit","Lovely book, it's bound tightly so you may not be able to add alot of photos/cards aside from the designated spaces in the book. Shop around before you purchase, as it is currently listed at Barnes & Noble for 29.95!","4"],["Baby Tracker&reg; - Daily Childcare Journal, Schedule Log","Perfect for new parents. We were able to keep track of baby's feeding, sleep and diaper change schedule for the first two and a half months of her life. Made life easier when the doctor would ask questions about habits because we had it all right there!","5"],["Baby Tracker&reg; - Daily Childcare Journal, Schedule Log","A friend of mine pinned this product on Pinterest so I decided to give it a whirl! It is fantastic! If you are a new parent, this will help you keep track of feedings, diaper changes and the like!","5"],["Baby Tracker&reg; - Daily Childcare Journal, Schedule Log","This has been an easy way for my nanny to record all the key events that happen with my baby when I'm not at home.  Would highly recommend it to someone who wants to stay informed of what your baby is up to while you're not home.The only reason this isn't a 5 is because I think there could have been some more standarad pre-printed options.I plan on ordering another one when we run out of pages in this journal.","4"]]},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:257"},{"text":"%md\n# Extract Features from raw data\n\nWe want to train a logistic regression for predicting the sentiment of a product review given its text. Since a logistic regression does not directly work with text, we need to extract some numeric features from the data.\n\nWe will perform the two following extractions, one for the label to be binary and a multi-step extraction for the features.\n\n1. Extract binary sentiment from given review value\n2. Extract bag-of-words model as features","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875490_-323050351","id":"20170129-052423_858609389","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Extract Features from raw data</h1>\n<p>We want to train a logistic regression for predicting the sentiment of a product review given its text. Since a logistic regression does not directly work with text, we need to extract some numeric features from the data.</p>\n<p>We will perform the two following extractions, one for the label to be binary and a multi-step extraction for the features.</p>\n<ol>\n<li>Extract binary sentiment from given review value</li>\n<li>Extract bag-of-words model as features</li>\n</ol>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:258"},{"text":"%md\n## 1. Extract Sentiment\n\nSince we want to perform a classification (positive review vs negative review), we need to extract a binary sentiment value. We will map the ratings as follows:\n\n* Ratings 1 and 2 count as a negative review\n* Rating 3 counts as a neutral review\n* Ratings 4 and 5 count as a positive review\n\nSince we want a binary classification, we will also remove neutral reviews altogether.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875490_-323050351","id":"20170129-042008_1516671555","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>1. Extract Sentiment</h2>\n<p>Since we want to perform a classification (positive review vs negative review), we need to extract a binary sentiment value. We will map the ratings as follows:</p>\n<ul>\n<li>Ratings 1 and 2 count as a negative review</li>\n<li>Rating 3 counts as a neutral review</li>\n<li>Ratings 4 and 5 count as a positive review</li>\n</ul>\n<p>Since we want a binary classification, we will also remove neutral reviews altogether.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259"},{"text":"// YOUR CODE HERE\nval data = ...","dateUpdated":"2017-01-31T09:53:15-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875490_-323050351","id":"20170129-042350_2056232695","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:260"},{"text":"%md\n## 2. Extract Features from Reviews\n\nNow we want to split the review text into individual words, so we can create a \"bag of words\" model. In order to get a somewhat nice model, we also need to remove all punctuations from the reviews.\n\n### 2.1. Remove Punctuations\nSo the first step is to remove all puncuations.  This will be done as the first step using a user defined function (UDF) in Spark, which will simply replace any punctuation character by a space.","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875491_-323435100","id":"20170129-043924_1480728312","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>2. Extract Features from Reviews</h2>\n<p>Now we want to split the review text into individual words, so we can create a &ldquo;bag of words&rdquo; model. In order to get a somewhat nice model, we also need to remove all punctuations from the reviews.</p>\n<h3>2.1. Remove Punctuations</h3>\n<p>So the first step is to remove all puncuations.  This will be done as the first step using a user defined function (UDF) in Spark, which will simply replace any punctuation character by a space.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:261"},{"text":"import org.apache.spark.sql.functions.udf\n\nval removePunctuation = udf { text:String => text.replaceAll(\"\\\\p{Punct}\", \" \") }\n// YOUR CODE HERE\nval data2 = ...","dateUpdated":"2017-01-31T09:55:08-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875491_-323435100","id":"20170129-043943_507827673","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:262"},{"text":"%md\n### 2.2 Split Reviews into Words\n\nNow we want to split every review into multiple words, resulting in a new data column containing an array with all words of the review. We could do that ourselves using some Scala methods, but we use a Transformer provided by Spark instead. Saves us some time and helps to create clean code.\n\nSpark ML contains a whole bunch of Transformers for extracting features. You can find them in the `org.apache.spark.ml.feature` package. Each Transformer accepts some parameters for specifying the details of the transformation. In particular most Transformers know about an input column and and output column which can be set via `setInputCol` and `setOutputCol`.\n\nOur specific task for splitting a string column into an array column with individual words can be performed with the `Tokenizer` Transform.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875491_-323435100","id":"20170129-043958_121242280","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>2.2 Split Reviews into Words</h3>\n<p>Now we want to split every review into multiple words, resulting in a new data column containing an array with all words of the review. We could do that ourselves using some Scala methods, but we use a Transformer provided by Spark instead. Saves us some time and helps to create clean code.</p>\n<p>Spark ML contains a whole bunch of Transformers for extracting features. You can find them in the <code>org.apache.spark.ml.feature</code> package. Each Transformer accepts some parameters for specifying the details of the transformation. In particular most Transformers know about an input column and and output column which can be set via <code>setInputCol</code> and <code>setOutputCol</code>.</p>\n<p>Our specific task for splitting a string column into an array column with individual words can be performed with the <code>Tokenizer</code> Transform.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:263"},{"text":"import org.apache.spark.ml.feature.Tokenizer\n\n// YOUR CODE HERE\nval tokenizer = ...\nval words = tokenizer.transform(data2)","dateUpdated":"2017-01-31T09:55:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"review","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"review","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875491_-323435100","id":"20170129-044010_2048325285","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:264"},{"text":"%md\n### 2.3 Remove Stop words\n\nWe also want to remove so called stop words, which are all those tiny words which mainly serve as glue for building sentences. Usually they do not contain much information in a simple bag of words model. So we get rid of them.\n\nThis is so common practice that Spark already contains a `StopWordsRemover` Transformer class for just doing that. Again we need to specify some parameters like `inputCol`, `outputCol` and `stopWords`.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875492_-325358844","id":"20170129-044024_1653567754","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>2.3 Remove Stop words</h3>\n<p>We also want to remove so called stop words, which are all those tiny words which mainly serve as glue for building sentences. Usually they do not contain much information in a simple bag of words model. So we get rid of them.</p>\n<p>This is so common practice that Spark already contains a <code>StopWordsRemover</code> Transformer class for just doing that. Again we need to specify some parameters like <code>inputCol</code>, <code>outputCol</code> and <code>stopWords</code>.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:265"},{"text":"val stopWords = Array(\"the\",\"a\",\"and\",\"or\", \"it\", \"this\", \"of\", \"an\", \"as\", \"in\", \"on\", \"is\", \"are\", \"to\", \"was\", \"for\", \"then\", \"i\", \"my\", \"that\", \"with\", \"we\", \"have\", \"so\", \"you\", \"s\", \"\")\n    \n// YOUR CODE HERE\nval stopWordsRemover = ...\nval vwords = stopWordsRemover.transform(words)","dateUpdated":"2017-01-31T09:55:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875492_-325358844","id":"20170129-044037_1562436005","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:266"},{"text":"%md\nNow lets have a look at common words. Maybe we want to change the list of stop words?","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875492_-325358844","id":"20170129-103148_909017422","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now lets have a look at common words. Maybe we want to change the list of stop words?</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:267"},{"text":"// YOUR CODE HERE\nval term_freq = ...\n\nz.show(term_freq.orderBy(col(\"count\").desc).limit(10))","dateUpdated":"2017-01-31T09:56:53-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875492_-325358844","id":"20170129-103147_26603517","result":{"code":"SUCCESS","type":"TABLE","msg":"word\tcount\nbut\t87326\nt\t84871\ns\t75227\nnot\t73570\nbaby\t64630\nthey\t61610\none\t60035\nvery\t57412\nwhen\t52551\ngreat\t52288\n","comment":"","msgTable":[[{"key":"count","value":"but"},{"key":"count","value":"87326"}],[{"value":"t"},{"value":"84871"}],[{"value":"s"},{"value":"75227"}],[{"value":"not"},{"value":"73570"}],[{"value":"baby"},{"value":"64630"}],[{"value":"they"},{"value":"61610"}],[{"value":"one"},{"value":"60035"}],[{"value":"very"},{"value":"57412"}],[{"value":"when"},{"value":"52551"}],[{"value":"great"},{"value":"52288"}]],"columnNames":[{"name":"word","index":0,"aggr":"sum"},{"name":"count","index":1,"aggr":"sum"}],"rows":[["but","87326"],["t","84871"],["s","75227"],["not","73570"],["baby","64630"],["they","61610"],["one","60035"],["very","57412"],["when","52551"],["great","52288"]]},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:268"},{"text":"%md\n### 2.4 Create Bag of Words Features\n\nFinally we simply count the number of occurances of all words within the reviews. Again we can simply use a `CountVectorizer` Transformer from the `org.apache.spark.ml.feature` package in Spark to perform that task. Again we need to specify `inputCol` and `outputCol`. Additionally we require that each word appears in at least two documents, to ensure some basic significance and avoid fitting the sentiment of a document onto single words.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875493_-325743593","id":"20170129-044054_945211724","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>2.4 Create Bag of Words Features</h3>\n<p>Finally we simply count the number of occurances of all words within the reviews. Again we can simply use a <code>CountVectorizer</code> Transformer from the <code>org.apache.spark.ml.feature</code> package in Spark to perform that task. Again we need to specify <code>inputCol</code> and <code>outputCol</code>. Additionally we require that each word appears in at least two documents, to ensure some basic significance and avoid fitting the sentiment of a document onto single words.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:269"},{"text":"import org.apache.spark.ml.feature.CountVectorizer\n\n// YOUR CODE HERE\nval countVectorizer = ...\n\nval countVectorizerModel = countVectorizer.fit(vwords)","dateUpdated":"2017-01-31T09:57:29-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875493_-325743593","id":"20170129-044111_370443742","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:270"},{"text":"%md\n### 2.4 Bag of Words - Inspect Vocabulary\n\nThe countVectorizerModel contains an implcit vocabulary containing all words. This can be useful for mapping features back to words\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875493_-325743593","id":"20170129-044132_249596213","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>2.4 Bag of Words - Inspect Vocabulary</h3>\n<p>The countVectorizerModel contains an implcit vocabulary containing all words. This can be useful for mapping features back to words</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:271"},{"text":"// YOUR CODE HERE","dateUpdated":"2017-01-31T09:51:59-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875493_-325743593","id":"20170129-044143_1368822013","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:272"},{"text":"%md\n### 2.5 Tidy up DataFrame\n\nWe now carry so many columns inside the DataFrame, let's remove some intermediate columns to get more focus on our model. In particular we remove the following columns:\n* `words`\n* `rating`\n \nWe keep the columns `review`, `name`, `sentiment` and `features`. For the model itself, we do not need `review` and `vwords`, but we will perform some simple checks with these columns later on.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875493_-325743593","id":"20170129-044155_1317352834","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>2.5 Tidy up DataFrame</h3>\n<p>We now carry so many columns inside the DataFrame, let's remove some intermediate columns to get more focus on our model. In particular we remove the following columns:</p>\n<ul>\n<li><code>words</code></li>\n<li><code>rating</code></li>\n</ul>\n<p>We keep the columns <code>review</code>, <code>name</code>, <code>sentiment</code> and <code>features</code>. For the model itself, we do not need <code>review</code> and <code>vwords</code>, but we will perform some simple checks with these columns later on.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:273"},{"text":"// YOUR CODE HERE","dateUpdated":"2017-01-31T09:52:27-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875494_-324589347","id":"20170129-044205_190711273","result":{"code":"SUCCESS","type":"TABLE","msg":"name\treview\tsentiment\tvwords\tfeatures\nPlanetwise Wipe Pouch\tit came early and was not disappointed  i love planet wise bags and now my wipe holder  it keps my osocozy wipes moist and does not leak  highly recommend it \t1.0\tWrappedArray(came, early, not, disappointed, , love, planet, wise, bags, now, my, wipe, holder, , keps, my, osocozy, wipes, moist, does, not, leak, , highly, recommend)\t(29579,[0,1,10,33,63,83,99,224,244,345,422,437,454,657,716,1051,2045,3564,3970,5094],[3.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\nAnnas Dream Full Quilt with 2 Shams\tVery soft and comfortable and warmer than it looks   fit the full size bed perfectly   would recommend to anyone looking for this type of quilt\t1.0\tWrappedArray(very, soft, comfortable, warmer, than, looks, , , fit, full, size, bed, perfectly, , , would, recommend, anyone, looking, type, quilt)\t(29579,[0,15,22,60,83,97,125,127,176,189,199,238,265,354,457,712,789,2669],[4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\nStop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book\tThis is a product well worth the purchase   I have not found anything else like this  and it is a positive  ingenious approach to losing the binky   What I love most about this product is how much ownership my daughter has in getting rid of the binky   She is so proud of herself  and loves her little fairy   I love the artwork  the chart in the back  and the clever approach of this tool \t1.0\tWrappedArray(product, well, worth, purchase, , , have, not, found, anything, else, like, , positive, , ingenious, approach, losing, binky, , , what, love, most, about, product, how, much, ownership, my, daughter, has, getting, rid, binky, , , she, so, proud, herself, , loves, her, little, fairy, , , love, artwork, , chart, back, , clever, approach, tool)\t(29579,[0,1,5,8,10,21,32,33,35,38,39,42,47,55,61,62,71,76,88,106,183,204,210,214,218,317,480,792,1644,1885,2099,2449,2975,3116,3523,3780,5119,6647,7068,7783,12259],[13.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])\nStop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book\tAll of my kids have cried non stop when I tried to ween them off their pacifier  until I found Thumbuddy To Love s Binky Fairy Puppet   It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from it This is a must buy book  and a great gift for expecting parents    You will save them soo many headaches Thanks for this book   You all rock  \t1.0\tWrappedArray(all, my, kids, have, cried, non, stop, when, tried, ween, them, off, their, pacifier, , until, found, thumbuddy, love, s, binky, fairy, puppet, , , easy, way, work, with, your, kids, allow, them, understand, where, their, pacifier, going, help, them, part, from, must, buy, book, , great, gift, expecting, parents, , , , you, will, save, them, soo, many, headaches, thanks, book, , , you, all, rock)\t(29579,[0,1,3,5,9,11,16,17,29,31,33,37,45,46,74,80,101,118,126,163,179,190,197,209,210,227,256,276,287,324,348,501,608,737,758,835,864,874,902,933,1193,1363,2640,2975,3585,6363,6647,6676,10423],[9.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\nStop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book\tWhen the Binky Fairy came to our house  we didn t have any special gift and book to help explain to her about how important it is to stop using a pacifier  This book does a great job to help prepare your child for the loss of their favorite item  The doll is adorable and we made lots of cute movies with the Binky Fairy telling our daughter about what happens when the Binky Fairy comes  I would highly recommend this product for any parent trying to break the pacifier or thumb sucking habit \t1.0\tWrappedArray(when, binky, fairy, came, our, house, , we, didn, t, have, any, special, gift, book, help, explain, her, about, how, important, stop, using, pacifier, , book, does, great, job, help, prepare, your, child, loss, their, favorite, item, , doll, adorable, we, made, lots, cute, movies, with, binky, fairy, telling, our, daughter, about, what, happens, when, binky, fairy, comes, , would, highly, recommend, product, any, parent, trying, break, pacifier, thumb, sucking, habit)\t(29579,[0,3,4,5,7,16,17,22,30,35,47,55,62,74,83,88,96,98,99,106,111,117,134,146,163,212,224,227,244,266,275,324,355,444,471,532,575,608,629,758,793,859,874,1205,1684,1714,1809,2975,2999,3429,3439,3543,3759,4069,6647,6955],[4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0])\nA Tale of Baby's Days with Peter Rabbit\tLovely book  it s bound tightly so you may not be able to add alot of photos cards aside from the designated spaces in the book  Shop around before you purchase  as it is currently listed at Barnes   Noble for 29 95 \t1.0\tWrappedArray(lovely, book, , s, bound, tightly, so, you, may, not, be, able, add, alot, photos, cards, aside, from, designated, spaces, book, , shop, around, before, you, purchase, , currently, listed, at, barnes, , , noble, 29, 95)\t(29579,[0,8,9,10,11,19,23,45,103,160,165,204,353,673,758,1208,1267,1545,1759,2051,2089,2157,2166,2191,2898,3787,3814,6435,7465,17121,18364],[5.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n","comment":"","msgTable":[[{"key":"review","value":"Planetwise Wipe Pouch"},{"key":"review","value":"it came early and was not disappointed  i love planet wise bags and now my wipe holder  it keps my osocozy wipes moist and does not leak  highly recommend it "},{"key":"review","value":"1.0"},{"key":"review","value":"WrappedArray(came, early, not, disappointed, , love, planet, wise, bags, now, my, wipe, holder, , keps, my, osocozy, wipes, moist, does, not, leak, , highly, recommend)"},{"key":"review","value":"(29579,[0,1,10,33,63,83,99,224,244,345,422,437,454,657,716,1051,2045,3564,3970,5094],[3.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"}],[{"key":"sentiment","value":"Annas Dream Full Quilt with 2 Shams"},{"key":"sentiment","value":"Very soft and comfortable and warmer than it looks   fit the full size bed perfectly   would recommend to anyone looking for this type of quilt"},{"key":"sentiment","value":"1.0"},{"key":"sentiment","value":"WrappedArray(very, soft, comfortable, warmer, than, looks, , , fit, full, size, bed, perfectly, , , would, recommend, anyone, looking, type, quilt)"},{"key":"sentiment","value":"(29579,[0,15,22,60,83,97,125,127,176,189,199,238,265,354,457,712,789,2669],[4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"}],[{"key":"vwords","value":"Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book"},{"key":"vwords","value":"This is a product well worth the purchase   I have not found anything else like this  and it is a positive  ingenious approach to losing the binky   What I love most about this product is how much ownership my daughter has in getting rid of the binky   She is so proud of herself  and loves her little fairy   I love the artwork  the chart in the back  and the clever approach of this tool "},{"key":"vwords","value":"1.0"},{"key":"vwords","value":"WrappedArray(product, well, worth, purchase, , , have, not, found, anything, else, like, , positive, , ingenious, approach, losing, binky, , , what, love, most, about, product, how, much, ownership, my, daughter, has, getting, rid, binky, , , she, so, proud, herself, , loves, her, little, fairy, , , love, artwork, , chart, back, , clever, approach, tool)"},{"key":"vwords","value":"(29579,[0,1,5,8,10,21,32,33,35,38,39,42,47,55,61,62,71,76,88,106,183,204,210,214,218,317,480,792,1644,1885,2099,2449,2975,3116,3523,3780,5119,6647,7068,7783,12259],[13.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])"}],[{"key":"features","value":"Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book"},{"key":"features","value":"All of my kids have cried non stop when I tried to ween them off their pacifier  until I found Thumbuddy To Love s Binky Fairy Puppet   It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from it This is a must buy book  and a great gift for expecting parents    You will save them soo many headaches Thanks for this book   You all rock  "},{"key":"features","value":"1.0"},{"key":"features","value":"WrappedArray(all, my, kids, have, cried, non, stop, when, tried, ween, them, off, their, pacifier, , until, found, thumbuddy, love, s, binky, fairy, puppet, , , easy, way, work, with, your, kids, allow, them, understand, where, their, pacifier, going, help, them, part, from, must, buy, book, , great, gift, expecting, parents, , , , you, will, save, them, soo, many, headaches, thanks, book, , , you, all, rock)"},{"key":"features","value":"(29579,[0,1,3,5,9,11,16,17,29,31,33,37,45,46,74,80,101,118,126,163,179,190,197,209,210,227,256,276,287,324,348,501,608,737,758,835,864,874,902,933,1193,1363,2640,2975,3585,6363,6647,6676,10423],[9.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"}],[{"value":"Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book"},{"value":"When the Binky Fairy came to our house  we didn t have any special gift and book to help explain to her about how important it is to stop using a pacifier  This book does a great job to help prepare your child for the loss of their favorite item  The doll is adorable and we made lots of cute movies with the Binky Fairy telling our daughter about what happens when the Binky Fairy comes  I would highly recommend this product for any parent trying to break the pacifier or thumb sucking habit "},{"value":"1.0"},{"value":"WrappedArray(when, binky, fairy, came, our, house, , we, didn, t, have, any, special, gift, book, help, explain, her, about, how, important, stop, using, pacifier, , book, does, great, job, help, prepare, your, child, loss, their, favorite, item, , doll, adorable, we, made, lots, cute, movies, with, binky, fairy, telling, our, daughter, about, what, happens, when, binky, fairy, comes, , would, highly, recommend, product, any, parent, trying, break, pacifier, thumb, sucking, habit)"},{"value":"(29579,[0,3,4,5,7,16,17,22,30,35,47,55,62,74,83,88,96,98,99,106,111,117,134,146,163,212,224,227,244,266,275,324,355,444,471,532,575,608,629,758,793,859,874,1205,1684,1714,1809,2975,2999,3429,3439,3543,3759,4069,6647,6955],[4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0])"}],[{"value":"A Tale of Baby's Days with Peter Rabbit"},{"value":"Lovely book  it s bound tightly so you may not be able to add alot of photos cards aside from the designated spaces in the book  Shop around before you purchase  as it is currently listed at Barnes   Noble for 29 95 "},{"value":"1.0"},{"value":"WrappedArray(lovely, book, , s, bound, tightly, so, you, may, not, be, able, add, alot, photos, cards, aside, from, designated, spaces, book, , shop, around, before, you, purchase, , currently, listed, at, barnes, , , noble, 29, 95)"},{"value":"(29579,[0,8,9,10,11,19,23,45,103,160,165,204,353,673,758,1208,1267,1545,1759,2051,2089,2157,2166,2191,2898,3787,3814,6435,7465,17121,18364],[5.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"}]],"columnNames":[{"name":"name","index":0,"aggr":"sum"},{"name":"review","index":1,"aggr":"sum"},{"name":"sentiment","index":2,"aggr":"sum"},{"name":"vwords","index":3,"aggr":"sum"},{"name":"features","index":4,"aggr":"sum"}],"rows":[["Planetwise Wipe Pouch","it came early and was not disappointed  i love planet wise bags and now my wipe holder  it keps my osocozy wipes moist and does not leak  highly recommend it ","1.0","WrappedArray(came, early, not, disappointed, , love, planet, wise, bags, now, my, wipe, holder, , keps, my, osocozy, wipes, moist, does, not, leak, , highly, recommend)","(29579,[0,1,10,33,63,83,99,224,244,345,422,437,454,657,716,1051,2045,3564,3970,5094],[3.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"],["Annas Dream Full Quilt with 2 Shams","Very soft and comfortable and warmer than it looks   fit the full size bed perfectly   would recommend to anyone looking for this type of quilt","1.0","WrappedArray(very, soft, comfortable, warmer, than, looks, , , fit, full, size, bed, perfectly, , , would, recommend, anyone, looking, type, quilt)","(29579,[0,15,22,60,83,97,125,127,176,189,199,238,265,354,457,712,789,2669],[4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"],["Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book","This is a product well worth the purchase   I have not found anything else like this  and it is a positive  ingenious approach to losing the binky   What I love most about this product is how much ownership my daughter has in getting rid of the binky   She is so proud of herself  and loves her little fairy   I love the artwork  the chart in the back  and the clever approach of this tool ","1.0","WrappedArray(product, well, worth, purchase, , , have, not, found, anything, else, like, , positive, , ingenious, approach, losing, binky, , , what, love, most, about, product, how, much, ownership, my, daughter, has, getting, rid, binky, , , she, so, proud, herself, , loves, her, little, fairy, , , love, artwork, , chart, back, , clever, approach, tool)","(29579,[0,1,5,8,10,21,32,33,35,38,39,42,47,55,61,62,71,76,88,106,183,204,210,214,218,317,480,792,1644,1885,2099,2449,2975,3116,3523,3780,5119,6647,7068,7783,12259],[13.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])"],["Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book","All of my kids have cried non stop when I tried to ween them off their pacifier  until I found Thumbuddy To Love s Binky Fairy Puppet   It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from it This is a must buy book  and a great gift for expecting parents    You will save them soo many headaches Thanks for this book   You all rock  ","1.0","WrappedArray(all, my, kids, have, cried, non, stop, when, tried, ween, them, off, their, pacifier, , until, found, thumbuddy, love, s, binky, fairy, puppet, , , easy, way, work, with, your, kids, allow, them, understand, where, their, pacifier, going, help, them, part, from, must, buy, book, , great, gift, expecting, parents, , , , you, will, save, them, soo, many, headaches, thanks, book, , , you, all, rock)","(29579,[0,1,3,5,9,11,16,17,29,31,33,37,45,46,74,80,101,118,126,163,179,190,197,209,210,227,256,276,287,324,348,501,608,737,758,835,864,874,902,933,1193,1363,2640,2975,3585,6363,6647,6676,10423],[9.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"],["Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book","When the Binky Fairy came to our house  we didn t have any special gift and book to help explain to her about how important it is to stop using a pacifier  This book does a great job to help prepare your child for the loss of their favorite item  The doll is adorable and we made lots of cute movies with the Binky Fairy telling our daughter about what happens when the Binky Fairy comes  I would highly recommend this product for any parent trying to break the pacifier or thumb sucking habit ","1.0","WrappedArray(when, binky, fairy, came, our, house, , we, didn, t, have, any, special, gift, book, help, explain, her, about, how, important, stop, using, pacifier, , book, does, great, job, help, prepare, your, child, loss, their, favorite, item, , doll, adorable, we, made, lots, cute, movies, with, binky, fairy, telling, our, daughter, about, what, happens, when, binky, fairy, comes, , would, highly, recommend, product, any, parent, trying, break, pacifier, thumb, sucking, habit)","(29579,[0,3,4,5,7,16,17,22,30,35,47,55,62,74,83,88,96,98,99,106,111,117,134,146,163,212,224,227,244,266,275,324,355,444,471,532,575,608,629,758,793,859,874,1205,1684,1714,1809,2975,2999,3429,3439,3543,3759,4069,6647,6955],[4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0])"],["A Tale of Baby's Days with Peter Rabbit","Lovely book  it s bound tightly so you may not be able to add alot of photos cards aside from the designated spaces in the book  Shop around before you purchase  as it is currently listed at Barnes   Noble for 29 95 ","1.0","WrappedArray(lovely, book, , s, bound, tightly, so, you, may, not, be, able, add, alot, photos, cards, aside, from, designated, spaces, book, , shop, around, before, you, purchase, , currently, listed, at, barnes, , , noble, 29, 95)","(29579,[0,8,9,10,11,19,23,45,103,160,165,204,353,673,758,1208,1267,1545,1759,2051,2089,2157,2166,2191,2898,3787,3814,6435,7465,17121,18364],[5.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"]]},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:274"},{"text":"%md\n# Split Train Data / Validation Data\n\nNow let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. Again the Spark DataFrame class offers a built in core method to perform the split.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875494_-324589347","id":"20170129-044218_1046591560","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Split Train Data / Validation Data</h1>\n<p>Now let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. Again the Spark DataFrame class offers a built in core method to perform the split.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:275"},{"text":"// YOUR CODE HERE\nval Array(trainingData, validationData) = ...\n\nprintln(s\"Training Data: ${trainingData.count()}\")\nprintln(s\"Validation Data: ${validationData.count()}\")","dateUpdated":"2017-01-31T09:57:59-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875494_-324589347","id":"20170129-044229_383362931","result":{"code":"SUCCESS","type":"TEXT","msg":"\n\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, review: string ... 2 more fields]\nvalidationData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, review: string ... 2 more fields]\nTraining Data: 127435\nValidation Data: 31917\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:276"},{"text":"%md\n# Train Classifier\n\nThere are many different classification algorithms out there. We will use a LogisticRegression, of course a DecisionTreeClassifier could be another interesting option.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875494_-324589347","id":"20170129-044249_824800015","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Train Classifier</h1>\n<p>There are many different classification algorithms out there. We will use a LogisticRegression, of course a DecisionTreeClassifier could be another interesting option.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:277"},{"text":"import org.apache.spark.ml.classification.LogisticRegression\n\n// YOUR CODE HERE\nval logisticRegression = ...\nval logisticModel = ...","dateUpdated":"2017-01-31T09:58:26-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875495_-324974096","id":"20170129-044302_1834217762","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:278"},{"text":"%md\n# Inspect Model\n\nThe LogisticRegressionModel also uses coefficients mapped to individual words. Let's have a look at them.\n\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875495_-324974096","id":"20170129-044314_1157019956","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Inspect Model</h1>\n<p>The LogisticRegressionModel also uses coefficients mapped to individual words. Let's have a look at them.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:279"},{"text":"logisticModel.coefficients.toArray.take(20).foreach(println)","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875495_-324974096","id":"20170129-044329_1169250287","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:280"},{"text":"%md\nLet's have a quick look about how many words have a positive weight and how many words have a negative weight.","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875495_-324974096","id":"20170129-101914_1671147041","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let's have a quick look about how many words have a positive weight and how many words have a negative weight.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:281"},{"text":"val coefficients = logisticModel.coefficients.toArray\nval numPositiveWeights = coefficients.count(_ > 0)\nval numNegativeWeights = coefficients.count(_ < 0)\n\nprintln(s\"Number positive weights $numPositiveWeights\")\nprintln(s\"Number negative weights $numNegativeWeights\")","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875495_-324974096","id":"20170129-044553_1980650936","result":{"code":"SUCCESS","type":"TEXT","msg":"coefficients: Array[Double] = Array(-0.017561437939671715, 0.295161682716593, 0.25021992730435205, 0.27733921214890384, 0.7107988554222496, 0.29477846424060833, -0.24605239408756102, -2.6303691532993927, 0.6363081830799967, 0.1592892960051039, -3.2170773563426023, -0.09822885696612584, -0.009343468661259131, 0.24760979861360516, 0.09872012291094587, -0.10360025738897112, 0.013817174757856548, 4.144903920455417, 1.1193100146109878, -0.32815204397575903, 0.7291297967074306, 0.9203937758281956, -0.794667308388645, -0.4282252876569603, -0.08061295048887308, -0.00676313387646479, -0.10343827564229385, 0.7125169451993731, -0.5386895485201948, -0.18634222624791286, 0.299533392201347, 0.24415239076737305, 1.2487307728678385, 5.546061728756544, 0.22028981833032443, 0.3299301473534606, -0.5563482...\nnumPositiveWeights: Int = 18132\n\nnumNegativeWeights: Int = 11137\nNumber positive weights 18132\nNumber negative weights 11137\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:282"},{"text":"%md\n## Find Weights of some Words\n\nLet's check how coefficients look like for some clearly positive or negative words. This can be done by first looking up a specific word in the vocabulary of the CountVectorizerModel and then using that index to retrieve the weight in the LogisticRegressionModel we just trained. ","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875496_-326897840","id":"20170129-044409_841287254","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Find Weights of some Words</h2>\n<p>Let's check how coefficients look like for some clearly positive or negative words. This can be done by first looking up a specific word in the vocabulary of the CountVectorizerModel and then using that index to retrieve the weight in the LogisticRegressionModel we just trained.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:283"},{"text":"def printWordWeight(word:String) = {\n    // YOUR CODE HERE\n    val index = ...\n    val weight = ...\n    println(s\"$word : $weight\")\n}\n    \n// YOUR CODE HERE    \n","dateUpdated":"2017-01-31T09:59:29-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875496_-326897840","id":"20170129-044413_362370312","result":{"code":"SUCCESS","type":"TEXT","msg":"\nprintWordWeight: (word: String)Unit\ngood : 1.6318796942307014\ngreat : 4.144903920455417\nbest : 4.13280592328618\nugly : 2.788884753667541\nworst : -11.463664764540798\ncheap : -2.4951556769690697\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:284"},{"text":"%md\n## Find Extreme Words\n\nLet us try to find the most positive and most negative word according to the weights. This can be achieved using a argmin function to find the index and the vocabulary to map the index to the actual word. Unfortunately Scala does not provide an argmin or argmax function out of the box, so we need to create our own.","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875496_-326897840","id":"20170129-044426_1753481605","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Find Extreme Words</h2>\n<p>Let us try to find the most positive and most negative word according to the weights. This can be achieved using a argmin function to find the index and the vocabulary to map the index to the actual word. Unfortunately Scala does not provide an argmin or argmax function out of the box, so we need to create our own.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:285"},{"text":"import scala.collection.IndexedSeq\nimport scala.reflect.ClassTag\n\ndef idxmin[X  <% Ordered[X]: ClassTag](seq:IndexedSeq[X]) = {\n    seq.zipWithIndex.minBy(_._1)._2\n}\ndef idxmax[X  <% Ordered[X]: ClassTag](seq:IndexedSeq[X]) = {\n    seq.zipWithIndex.maxBy(_._1)._2\n}","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875497_-327282589","id":"20170129-062847_749373872","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:286"},{"text":"%md\nWith argmin and argmax in place, we can now find the best and worst words","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875497_-327282589","id":"20170129-062010_1045358891","result":{"code":"SUCCESS","type":"HTML","msg":"<p>With argmin and argmax in place, we can now find the best and worst words</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:287"},{"text":"//val worstWordIndex = argmin(logisticModel.coefficients.toArray.indices, { n:Int => logisticModel.coefficients.toArray(n)} )\nval worstWordIndex = idxmin(logisticModel.coefficients.toArray)\nval worstWord = countVectorizerModel.vocabulary(worstWordIndex)\nval worstWeight = logisticModel.coefficients(worstWordIndex)\nprintln(s\"Worst word: $worstWord value $worstWeight\")\n\n//val bestWordIndex = argmax(logisticModel.coefficients.toArray.indices, { n:Int => logisticModel.coefficients.toArray(n)} )\nval bestWordIndex = idxmax(logisticModel.coefficients.toArray)\nval bestWord = countVectorizerModel.vocabulary(bestWordIndex)\nval bestWeight = logisticModel.coefficients(bestWordIndex)\nprintln(s\"Best word: $bestWord value $bestWeight\")","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875497_-327282589","id":"20170129-044442_1516043671","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:288"},{"text":"%md\n## Document Frequencies\n\nLet's have a look at document frequencies of words","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875497_-327282589","id":"20170129-064007_1643161693","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Document Frequencies</h2>\n<p>Let's have a look at document frequencies of words</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289"},{"text":"import org.apache.spark.sql.functions.explode\n\nval doc_freq = features\n    .select(explode(features(\"vwords\")).alias(\"word\"), features(\"review\"))\n    .distinct()\n    .groupBy(col(\"word\")).count()\n\nz.show(doc_freq.orderBy(col(\"count\").desc).limit(10))","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875497_-327282589","id":"20170129-044643_486786656","result":{"code":"SUCCESS","type":"TABLE","msg":"word\tcount\n\t154689\nmy\t90945\nthat\t63055\nwith\t62874\nbut\t59980\nhave\t56813\nso\t55520\nt\t52971\nnot\t48931\ns\t46585\n","comment":"","msgTable":[[{"key":"count","value":""},{"key":"count","value":"154689"}],[{"value":"my"},{"value":"90945"}],[{"value":"that"},{"value":"63055"}],[{"value":"with"},{"value":"62874"}],[{"value":"but"},{"value":"59980"}],[{"value":"have"},{"value":"56813"}],[{"value":"so"},{"value":"55520"}],[{"value":"t"},{"value":"52971"}],[{"value":"not"},{"value":"48931"}],[{"value":"s"},{"value":"46585"}]],"columnNames":[{"name":"word","index":0,"aggr":"sum"},{"name":"count","index":1,"aggr":"sum"}],"rows":[["","154689"],["my","90945"],["that","63055"],["with","62874"],["but","59980"],["have","56813"],["so","55520"],["t","52971"],["not","48931"],["s","46585"]]},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:290"},{"text":"%md\n# Making Predictions\n\nThe primary idea is of course to make predictions of the sentiment using the learned model.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875498_-326128342","id":"20170129-044654_1617813197","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Making Predictions</h1>\n<p>The primary idea is of course to make predictions of the sentiment using the learned model.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:291"},{"text":"// YOUR CODE HERE\nval pred = ...\n","dateUpdated":"2017-01-31T10:02:01-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875498_-326128342","id":"20170129-044704_1120754047","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:292"},{"text":"%md\n# Find the most Positive Review\n\nUsing the column rawPrediction, we can find the review which has the highest positive prediction.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875498_-326128342","id":"20170129-044715_63959255","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Find the most Positive Review</h1>\n<p>Using the column rawPrediction, we can find the review which has the highest positive prediction.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:293"},{"text":"import org.apache.spark.ml.linalg.Vector\n// Extract one component from a Vectors\nval extractFromVector = udf{ (v:Vector, i:Integer) => v(i) }\n\n// YOUR CODE HERE\nval positives = ...\n\nz.show()\n    positives.select(col(\"name\"),col(\"review\"),col(\"sentiment\"),col(\"rawPrediction\"),col(\"prediction\")).limit(6)\n)    ","dateUpdated":"2017-01-31T10:03:13-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"review","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"review","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875498_-326128342","id":"20170129-044725_1084497064","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:294"},{"text":"%md\n# Evaluation of Prediction\n\nAgain we want to assess the performance of the prediction model. This can be done using the builtin class BinaryClassificationEvaluator.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875499_-326513091","id":"20170129-044740_1008352335","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Evaluation of Prediction</h1>\n<p>Again we want to assess the performance of the prediction model. This can be done using the builtin class BinaryClassificationEvaluator.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:295"},{"text":"import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n\n// YOUR CODE HERE\nval evaluator = ...\nprintln(evaluator.evaluate(pred))\n","dateUpdated":"2017-01-31T10:03:48-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875499_-326513091","id":"20170129-044751_2122449463","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:296"},{"text":"%md\n# Custom Evaluator\n\nWe want to use a different metric namely accuracy. Accuracy is defined as\n\nnumber_correct_predictions / total_number_predictions\n\nFirst let us directly calculate that metric\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875499_-326513091","id":"20170129-044803_1942847520","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Custom Evaluator</h1>\n<p>We want to use a different metric namely accuracy. Accuracy is defined as</p>\n<p>number_correct_predictions / total_number_predictions</p>\n<p>First let us directly calculate that metric</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:297"},{"text":"// YOUR CODE HERE\nval numTotal = ...\nval numCorrect = ...\n\nval modelAccuracy = numCorrect.toFloat / numTotal.toFloat\n\nprintln(s\"Model Accuracy: $modelAccuracy\")","dateUpdated":"2017-01-31T10:04:42-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875499_-326513091","id":"20170129-044811_586986077","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:298"},{"text":"%md\n## Compare with Dummy Predictor\n\nIt is always interesting to see how a trivial prediction performs. The trivial predictor simply predicts the most common class for all objects. In this case this would be a positive review.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044827_1506840292","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Compare with Dummy Predictor</h2>\n<p>It is always interesting to see how a trivial prediction performs. The trivial predictor simply predicts the most common class for all objects. In this case this would be a positive review.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:299"},{"text":"// YOUR CODE HERE\nval numTotal = ...\nval numPositive = ...\n\nval baselineAccuracy = numPositive.toFloat / numTotal\n\nprintln(s\"Baseline Accuracy: $baselineAccuracy\")","dateUpdated":"2017-01-31T10:04:42-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044840_1725986384","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:300"},{"text":"%md\n## Custom Evaluator\n\nNow let us create a new Evaluator class implementing accuracy as the relevant Metric.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044900_1965068259","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Custom Evaluator</h2>\n<p>Now let us create a new Evaluator class implementing accuracy as the relevant Metric.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:301"},{"text":"import org.apache.spark.sql.functions.col\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.ml.param.Param\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.evaluation.Evaluator\n\nclass AccuracyClassificationEvaluator(override val uid: String) extends org.apache.spark.ml.evaluation.Evaluator {\n    \n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val labelCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"labelCol\", \"label column name\")\n    setDefault(labelCol, \"label\")\n    def getLabelCol: String = $(labelCol)\n    def setLabelCol(value:String): this.type = set(labelCol, value)\n\n    val predictionCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"predictionCol\", \"prediction column name\")\n    setDefault(predictionCol, \"prediction\")\n    def getPredictionCol: String = $(predictionCol)\n    def setPredictionCol(value:String): this.type = set(predictionCol, value)\n\n    override def evaluate(dataset: org.apache.spark.sql.Dataset[_]) : Double = {\n        val numTotal = dataset.count()\n        val numCorrect = dataset.filter(dataset(getLabelCol) === dataset(getPredictionCol)).count()\n        numCorrect.toDouble / numTotal\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): AccuracyClassificationEvaluator = null // defaultCopy(extra)\n\n}","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044909_2137892823","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:302"},{"text":"// YOUR CODE HERE\nval evaluator = \n\nevaluator.evaluate(pred)","dateUpdated":"2017-01-31T10:05:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044928_1012472330","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:303"},{"text":"%md\n# Tweak Hyper Parameters\n\nAgain we want to improve overall performance by tweaking model parameters. So first let's see which parameters are available for tweaking\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044938_1518598932","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Tweak Hyper Parameters</h1>\n<p>Again we want to improve overall performance by tweaking model parameters. So first let's see which parameters are available for tweaking</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:304"},{"text":"println(new LogisticRegression().explainParams())","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875501_-328821585","id":"20170129-044943_1471930743","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:305"},{"text":"%md\nLet us try some different parameters and check the results","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875501_-328821585","id":"20170129-044953_1947443369","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let us try some different parameters and check the results</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:306"},{"text":"val logisticRegression2 = new LogisticRegression()\n    .setFeaturesCol(\"features\")\n    .setLabelCol(\"sentiment\")\n    .setRegParam(0.01)\n    .setMaxIter(100)\nval logisticModel2 = logisticRegression2.fit(train_data)\n\nval pred = logisticModel2.transform(test_data)\n\nval roc_evaluator = new BinaryClassificationEvaluator()\n    .setLabelCol(\"sentiment\")\n    .setMetricName(\"areaUnderROC\")\nval acc_evaluator = new AccuracyClassificationEvaluator()\n    .setLabelCol(\"sentiment\")\n\nprintln(s\"areaUnderROC = ${roc_evaluator.evaluate(pred)}\")\nprintln(s\"accuracy = ${acc_evaluator.evaluate(pred)}\")","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875501_-328821585","id":"20170129-045003_1169230278","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:307"},{"text":"%md\n## Finding best Hyper Parameters\n\nSo we got an improvement, but what would be best? We need to try.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875501_-328821585","id":"20170129-045014_296514678","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Finding best Hyper Parameters</h2>\n<p>So we got an improvement, but what would be best? We need to try.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:308"},{"text":"for (reg_param <- Array(0.0, 0.0001, 0.01, 1.0, 100.0)) {\n    val logisticRegression2 = new LogisticRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"sentiment\")\n        .setRegParam(reg_param)\n        .setMaxIter(100)\n        \n    // YOUR CODE HERE        \n    val logisticModel2 = ...\n    \n    val pred = ...\n    \n    val roc_evaluator = new BinaryClassificationEvaluator()\n        .setLabelCol(\"sentiment\")\n        .setMetricName(\"areaUnderROC\")\n    val acc_evaluator = new AccuracyClassificationEvaluator()\n        .setLabelCol(\"sentiment\")\n\n    val roc = ...\n    val acc = ...\n    println(s\"reg_param = $reg_param\")\n    println(s\"    areaUnderROC = ${roc}\")\n    println(s\"    Model Accuracy = ${acc}\")\n}","dateUpdated":"2017-01-31T10:07:22-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875501_-328821585","id":"20170129-045023_455651463","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:309"},{"text":"%md\n## ParamGridBuilder & CrossValidator\n\nSince the selection of hyper parameters is a very common job and might be tedious work, there is some nice support in PySpark to simplify it. It is a two-step approach:\n\n* Use ParamGridBuilder to create a set of parameters to test, possibly for different hyper parameters\n* Use a CrossValidator for selecting the best set of parameters\n\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875502_-327667338","id":"20170129-045036_716307760","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>ParamGridBuilder &amp; CrossValidator</h2>\n<p>Since the selection of hyper parameters is a very common job and might be tedious work, there is some nice support in PySpark to simplify it. It is a two-step approach:</p>\n<ul>\n<li>Use ParamGridBuilder to create a set of parameters to test, possibly for different hyper parameters</li>\n<li>Use a CrossValidator for selecting the best set of parameters</li>\n</ul>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:310"},{"text":"import org.apache.spark.ml.tuning.ParamGridBuilder\n\nval lr = new LogisticRegression()\n    .setFeaturesCol(\"features\")\n    .setLabelCol(\"sentiment\")\nval param_grid = new ParamGridBuilder()\n    .addGrid(lr.regParam, Array(0.0, 0.0001, 0.01, 1.0, 100.0))\n    .addGrid(lr.maxIter, Array(10, 100))\n    .build()\n    \nfor (pset <- param_grid) {\n    println(pset.toSeq.map(p => s\"${p.param.name}=${p.value}\").mkString(\", \"))\n}","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875502_-327667338","id":"20170129-045049_988951344","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:311"},{"text":"import org.apache.spark.ml.tuning.CrossValidator\n\nval lr = new LogisticRegression()\n    .setFeaturesCol(\"features\")\n    .setLabelCol(\"sentiment\")\nval evaluator = new AccuracyClassificationEvaluator()\n    .setLabelCol(\"sentiment\")\nval cv = new CrossValidator()\n    .setEstimator(lr)\n    .setEstimatorParamMaps(param_grid)\n    .setEvaluator(evaluator)\n    .setNumFolds(3)\nval model = cv.fit(train_data)\n\nprintln(evaluator.evaluate(model.transform(test_data)))","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875502_-327667338","id":"20170129-045113_1186994890","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:312"},{"text":"model.bestModel.params.foreach(p => println(p.name + \"=\" + model.bestModel.get(p).getOrElse(None)))","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875502_-327667338","id":"20170129-075356_1634607010","result":{"code":"SUCCESS","type":"TEXT","msg":"elasticNetParam=None\nfeaturesCol=features\nfitIntercept=None\nlabelCol=sentiment\nmaxIter=100\npredictionCol=None\nprobabilityCol=None\nrawPredictionCol=None\nregParam=0.01\nstandardization=None\nthreshold=None\nthresholds=None\ntol=None\nweightCol=None\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:313"},{"text":"","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875502_-327667338","id":"20170129-080247_746419066","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:314"}],"name":"Amazon Baby Products Classification Skeleton","id":"2C91GE1A4","angularObjects":{"2C73P7NJN:shared_process":[],"2C7KU6EWG:shared_process":[],"2C8H7AG7Q:shared_process":[],"2CANY5QMM:shared_process":[],"2C7YM9SBT:shared_process":[],"2CAHZM5EW:shared_process":[],"2CA194TC4:shared_process":[],"2C85Z5A3J:shared_process":[],"2C77BBC6M:shared_process":[],"2C9T8R64M:shared_process":[],"2C82H3SUX:shared_process":[],"2C7W1UTSM:shared_process":[],"2C99CAHNC:shared_process":[],"2C7VKNJZ3:shared_process":[],"2C7MNAP62:shared_process":[],"2C8SJ4SC1:shared_process":[],"2C8273BS9:shared_process":[],"2CA9V89Q3:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}