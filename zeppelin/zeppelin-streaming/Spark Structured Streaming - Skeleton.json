{"paragraphs":[{"text":"%md\n# 1. Connect to data source\n\nFirst you need to run the netcat program, for example via\n\n    pynetcat.py -T -I1 -B10 -P9977 < alice-in-wonderland.txt\n\nAlternatively you can also stream from S3 using\n\n    s3netcat.py -T -I1 -B10 -P9977 s3://dimajix-training/data/alice/alice-in-wonderland.txt\n\nThen we connect to the raw data socket as the datasource by using the `DataStreamReader` API via `spark.readStream`. We need to specify the options `host`, `port` and we need to use the format `socket` for connecting to the data source. The socket will stream weather data samples in raw format, i.e. one record per line.","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092592_-1216556133","id":"20170218-160028_195174762","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>1. Connect to data source</h1>\n<p>First you need to run the netcat program, for example via</p>\n<pre><code>pynetcat.py -T -I1 -B10 -P9977 &lt; alice-in-wonderland.txt\n</code></pre>\n<p>Alternatively you can also stream from S3 using</p>\n<pre><code>s3netcat.py -T -I1 -B10 -P9977 s3://dimajix-training/data/alice/alice-in-wonderland.txt\n</code></pre>\n<p>Then we connect to the raw data socket as the datasource by using the <code>DataStreamReader</code> API via <code>spark.readStream</code>. We need to specify the options <code>host</code>, <code>port</code> and we need to use the format <code>socket</code> for connecting to the data source. The socket will stream weather data samples in raw format, i.e. one record per line.</p>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2295"},{"text":"// Fill in the correct AWS VPC address of your master host\nval master = \"10.200.101.195\"\n\n// Connect to raw text stream socket using the DataStreamReader API via spark.readStream. You need to specify the options `host`, `port` and you need to use the format `socket`\nval lines = ...","dateUpdated":"2017-02-20T17:53:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092593_-1216940882","id":"20170218-160002_129671727","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2296","focus":true},{"text":"%md\n## 1.1 Inspect Schema\n\nThe result of the load method is a `DataFrame` again, but a streaming one. This `DataFrame` again has a schema, which we can inspect with the usual method:","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092594_-1215786635","id":"20170218-161504_128762750","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>1.1 Inspect Schema</h2>\n<p>The result of the load method is a <code>DataFrame</code> again, but a streaming one. This <code>DataFrame</code> again has a schema, which we can inspect with the usual method:</p>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2297"},{"text":"lines.printSchema()","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092595_-1216171384","id":"20170218-160206_920542952","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2298","focus":true},{"text":"%md\n## 1.2 Extract Timestamp\n\nThis time a timestamp is attached to every line. We want to extract the timestamp and handle it separately.","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092595_-1216171384","id":"20170218-165528_130307776","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>1.2 Extract Timestamp</h2>\n<p>This time a timestamp is attached to every line. We want to extract the timestamp and handle it separately.</p>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2299"},{"text":"import org.apache.spark.sql.types.TimestampType\n\nval ts_lines = ...","dateUpdated":"2017-02-20T17:53:25+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092595_-1216171384","id":"20170218-165525_1370327040","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2300","focus":true},{"text":"ts_lines.printSchema","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092595_-1216171384","id":"20170218-165602_1908375523","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2301","focus":true},{"text":"%md\n# 2. Inspect Data\n\nOf course we also want to inspect the data inside the DataFrame. But this time, we cannot simply invoke `show`, because normal actions do not (directly) work on streaming DataFrames. Instead we need to create a continiuous query. Later, we will see a neat trick how a streaming query can be transformed into a volatile table.","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092596_-1218095129","id":"20170218-161603_528321172","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>2. Inspect Data</h1>\n<p>Of course we also want to inspect the data inside the DataFrame. But this time, we cannot simply invoke <code>show</code>, because normal actions do not (directly) work on streaming DataFrames. Instead we need to create a continiuous query. Later, we will see a neat trick how a streaming query can be transformed into a volatile table.</p>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2302"},{"text":"val query = ...","dateUpdated":"2017-02-20T17:53:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092596_-1218095129","id":"20170218-160132_1015574950","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2303","focus":true},{"text":"%md\n## 2.1 Stop Query\n\nIn contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the `stop` method on the query object. This makes working with streams much easier.","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092596_-1218095129","id":"20170218-161746_736484246","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>2.1 Stop Query</h2>\n<p>In contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the <code>stop</code> method on the query object. This makes working with streams much easier.</p>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2304"},{"text":"","dateUpdated":"2017-02-20T17:53:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092596_-1218095129","id":"20170218-160155_661067655","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2305"},{"text":"%md\n# 3. Counting Words\n\nSo we now want to create a streaming word count. We perform the same actions as in the traditional DataFrame word count example, i.e. we split every line into words, group the words and count the sizes of the groups.","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092596_-1218095129","id":"20170218-161837_1521722724","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>3. Counting Words</h1>\n<p>So we now want to create a streaming word count. We perform the same actions as in the traditional DataFrame word count example, i.e. we split every line into words, group the words and count the sizes of the groups.</p>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2306"},{"text":"val words = ...","dateUpdated":"2017-02-20T17:53:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092596_-1218095129","id":"20170218-160238_1475524112","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2307","focus":true},{"text":"val counts = ...","dateUpdated":"2017-02-20T17:53:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092596_-1218095129","id":"20170218-160526_2008806660","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2308","focus":true},{"text":"","dateUpdated":"2017-02-20T17:53:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092596_-1218095129","id":"20170218-162308_1835168589","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2309","focus":true},{"text":"%md\n# 4. Print Results onto Console\n\nAgain we want to print the results onto the console.","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092597_-1218479877","id":"20170218-161041_735302634","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>4. Print Results onto Console</h1>\n<p>Again we want to print the results onto the console.</p>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2310"},{"text":"val query = ...","dateUpdated":"2017-02-20T17:53:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092597_-1218479877","id":"20170218-160613_988660270","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2311","focus":true},{"text":"","dateUpdated":"2017-02-20T17:53:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092597_-1218479877","id":"20170218-160716_194527039","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2312"},{"text":"%md\n# 5. Time-Windowed Aggregation\n\nAnother interesting (and probably more realistic) application is to perform time windowed aggregations. This means that we define a sliding time window used in the `groupBy` clause. In addition we also define a so called *watermark* which tells Spark how long to wait for late arrivels of individual data points (we don't have them in our simple example).","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092597_-1218479877","id":"20170218-164639_1471935006","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>5. Time-Windowed Aggregation</h1>\n<p>Another interesting (and probably more realistic) application is to perform time windowed aggregations. This means that we define a sliding time window used in the <code>groupBy</code> clause. In addition we also define a so called <em>watermark</em> which tells Spark how long to wait for late arrivels of individual data points (we don't have them in our simple example).</p>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2313"},{"text":"val windowedCounts = ...","dateUpdated":"2017-02-20T17:53:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092597_-1218479877","id":"20170218-164703_2039341008","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2314","focus":true},{"text":"","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092597_-1218479877","id":"20170218-165357_1483843188","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2315","focus":true},{"text":"%md\n# 5. Create Dynamic Table\n\nWe can also use a \"memory\" output, which is a queryable live table. In order to do so, we again create a new table, but this time with format `memory` and an explicit query name `aggregated_weather`. Using a `memory` output will create a dynamic table in memory (only `complete` output supported right now), which can be queried using SQL.\n\n1. Create a DataStreamWriter object using the writeStream method of your DataFrame `windowedCounts`.\n2. Set the format to `memory`\n3. Set the output mode to `append` (this is supported for time windowed aggregations)\n4. Set the query name to `alice_counts`\n5. Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)\n6. Start the continuous query via `start`","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092597_-1218479877","id":"20170218-160934_1425298948","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>5. Create Dynamic Table</h1>\n<p>We can also use a &ldquo;memory&rdquo; output, which is a queryable live table. In order to do so, we again create a new table, but this time with format <code>memory</code> and an explicit query name <code>aggregated_weather</code>. Using a <code>memory</code> output will create a dynamic table in memory (only <code>complete</code> output supported right now), which can be queried using SQL.</p>\n<ol>\n<li>Create a DataStreamWriter object using the writeStream method of your DataFrame <code>windowedCounts</code>.</li>\n<li>Set the format to <code>memory</code></li>\n<li>Set the output mode to <code>append</code> (this is supported for time windowed aggregations)</li>\n<li>Set the query name to <code>alice_counts</code></li>\n<li>Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)</li>\n<li>Start the continuous query via <code>start</code></li>\n</ol>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2316"},{"text":"val tableQuery = ...","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092598_-1217325631","id":"20170218-161145_208429016","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2317","focus":true},{"text":"%md\n## 5.1 Perform Query\n\nNow that we have a dynamic table, we can perform SQL queries against this table as if it was a normal static table.","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092598_-1217325631","id":"20170218-162010_1110788275","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>5.1 Perform Query</h2>\n<p>Now that we have a dynamic table, we can perform SQL queries against this table as if it was a normal static table.</p>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2318"},{"text":"%sql\n","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"window","index":0,"aggr":"sum"}],"values":[{"name":"word","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"window","index":0,"aggr":"sum"},"yAxis":{"name":"word","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092598_-1217325631","id":"20170218-161337_1103351127","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2319","focus":true},{"text":"%md\n## 5.2 Stop Query\n\nIn order to clean everything up, we stop the query again.","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092605_-1221557869","id":"20170218-162048_842521010","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>5.2 Stop Query</h2>\n<p>In order to clean everything up, we stop the query again.</p>\n"},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2320"},{"text":"","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092605_-1221557869","id":"20170218-161329_1844676678","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2321"},{"text":"","dateUpdated":"2017-02-20T17:51:32+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487613092605_-1221557869","id":"20170218-162247_324928954","dateCreated":"2017-02-20T17:51:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2322"}],"name":"Spark Structured Streaming - Skeleton","id":"2C9BRTP4D","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}