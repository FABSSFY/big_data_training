{"paragraphs":[{"text":"%md\n# 1. Connect to data source\n\nFirst you need to run the netcat program, for example via\n\n    s3netcat.py -T -I1 -B10 -P9977 s3://dimajix-training/data/twitter-sample/\n\nThen we connect to the raw data socket as the datasource by using the `DataStreamReader` API via `spark.readStream`. We need to specify the options `host`, `port` and we need to use the format `socket` for connecting to the data source. The socket will stream Twitter data samples in raw JSON format, i.e. one JSON document per line.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402897_261157647","id":"20170218-160028_195174762","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>1. Connect to data source</h1>\n<p>First you need to run the netcat program, for example via</p>\n<pre><code>s3netcat.py -T -I1 -B10 -P9977 s3://dimajix-training/data/twitter-sample/\n</code></pre>\n<p>Then we connect to the raw data socket as the datasource by using the <code>DataStreamReader</code> API via <code>spark.readStream</code>. We need to specify the options <code>host</code>, <code>port</code> and we need to use the format <code>socket</code> for connecting to the data source. The socket will stream Twitter data samples in raw JSON format, i.e. one JSON document per line.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7174"},{"text":"// Fill in the correct AWS VPC address of your master host\nval master = ...\n\n// Connect to raw text stream socket using the DataStreamReader API via spark.readStream. You need to specify the options `host`, `port` and you need to use the format `socket`\nval lines = ...","dateUpdated":"2017-02-20T18:32:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402897_261157647","id":"20170218-160002_129671727","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7175","focus":true},{"text":"%md\n## 1.1 Inspect Schema\n\nThe result of the load method is a `DataFrame` again, but a streaming one. This `DataFrame` again has a schema, which we can inspect with the usual method:","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402897_261157647","id":"20170218-161504_128762750","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>1.1 Inspect Schema</h2>\n<p>The result of the load method is a <code>DataFrame</code> again, but a streaming one. This <code>DataFrame</code> again has a schema, which we can inspect with the usual method:</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7176"},{"text":"","dateUpdated":"2017-02-20T18:32:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402897_261157647","id":"20170218-160206_920542952","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7177","focus":true},{"text":"%md\n## 1.2 Extract Timestamp\n\nThis time a timestamp is attached to every line. We want to extract the timestamp and handle it separately.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402897_261157647","id":"20170218-165528_130307776","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>1.2 Extract Timestamp</h2>\n<p>This time a timestamp is attached to every line. We want to extract the timestamp and handle it separately.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7178"},{"text":"import org.apache.spark.sql.types.TimestampType\n\nval ts_records = ...\n","dateUpdated":"2017-02-20T18:32:13+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402897_261157647","id":"20170218-165525_1370327040","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7179","focus":true},{"text":"ts_records.printSchema","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402897_261157647","id":"20170218-165602_1908375523","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7180","focus":true},{"text":"%md\n# 2. Inspect Data\n\nOf course we also want to inspect the data inside the DataFrame. But this time, we cannot simply invoke `show`, because normal actions do not (directly) work on streaming DataFrames. Instead we need to create a continiuous query. Later, we will see a neat trick how a streaming query can be transformed into a volatile table.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402897_261157647","id":"20170218-161603_528321172","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>2. Inspect Data</h1>\n<p>Of course we also want to inspect the data inside the DataFrame. But this time, we cannot simply invoke <code>show</code>, because normal actions do not (directly) work on streaming DataFrames. Instead we need to create a continiuous query. Later, we will see a neat trick how a streaming query can be transformed into a volatile table.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7181"},{"text":"val query = ...","dateUpdated":"2017-02-20T18:32:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170218-160132_1015574950","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7182","focus":true},{"text":"%md\n## 2.1 Stop Query\n\nIn contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the `stop` method on the query object. This makes working with streams much easier.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170218-161746_736484246","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>2.1 Stop Query</h2>\n<p>In contrast to the RDD API, we can simply stop an individual query instead of a whole StreamingContext by simply calling the <code>stop</code> method on the query object. This makes working with streams much easier.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7183"},{"text":"","dateUpdated":"2017-02-20T18:32:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170218-160155_661067655","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7184"},{"text":"%md\n# 3. Counting Hash-Tags\n\nSo we now want to create a streaming hashtag count. First we need to extract the Tweet itself from the JSON document, then we need to extract the hashtags in a similar way to the batch word traditional DataFrame word count example, i.e. we split every line into words, keep only hash-tags, group the words and count the sizes of the groups.\n\nEach query looks as follows\n\n```\n{ \"contributors\" : null,\n  \"coordinates\" : null,\n  \"created_at\" : \"Fri Jul 29 12:46:00 +0000 2016\",\n  \"entities\" : { \"hashtags\" : [  ],\n      \"symbols\" : [  ],\n      \"urls\" : [ { \"display_url\" : \"fb.me/ItnwZEhy\",\n            \"expanded_url\" : \"http://fb.me/ItnwZEhy\",\n            \"indices\" : [ 33,\n                56\n              ],\n            \"url\" : \"https://t.co/mM0if95F1K\"\n          } ],\n      \"user_mentions\" : [  ]\n    },\n  \"favorite_count\" : 0,\n  \"favorited\" : false,\n  \"filter_level\" : \"low\",\n  \"geo\" : null,\n  \"id\" : 759007065155117058,\n  \"id_str\" : \"759007065155117058\",\n  \"in_reply_to_screen_name\" : null,\n  \"in_reply_to_status_id\" : null,\n  \"in_reply_to_status_id_str\" : null,\n  \"in_reply_to_user_id\" : null,\n  \"in_reply_to_user_id_str\" : null,\n  \"is_quote_status\" : false,\n  \"lang\" : \"en\",\n  \"place\" : null,\n  \"possibly_sensitive\" : false,\n  \"retweet_count\" : 0,\n  \"retweeted\" : false,\n  \"source\" : \"<a href=\\\"http://www.facebook.com/twitter\\\" rel=\\\"nofollow\\\">Facebook</a>\",\n  \"text\" : \"I posted a new video to Facebook https://t.co/mM0if95F1K\",\n  \"timestamp_ms\" : \"1469796360659\",\n  \"truncated\" : false,\n  \"user\" : { \"contributors_enabled\" : false,\n      \"created_at\" : \"Sat Sep 08 08:28:55 +0000 2012\",\n      \"default_profile\" : false,\n      \"default_profile_image\" : false,\n      \"description\" : null,\n      \"favourites_count\" : 0,\n      \"follow_request_sent\" : null,\n      \"followers_count\" : 0,\n      \"following\" : null,\n      \"friends_count\" : 0,\n      \"geo_enabled\" : false,\n      \"id\" : 810489374,\n      \"id_str\" : \"810489374\",\n      \"is_translator\" : false,\n      \"lang\" : \"zh-tw\",\n      \"listed_count\" : 0,\n      \"location\" : null,\n      \"name\" : \"張冥閻\",\n      \"notifications\" : null,\n      \"profile_background_color\" : \"FFF04D\",\n      \"profile_background_image_url\" : \"http://abs.twimg.com/images/themes/theme19/bg.gif\",\n      \"profile_background_image_url_https\" : \"https://abs.twimg.com/images/themes/theme19/bg.gif\",\n      \"profile_background_tile\" : false,\n      \"profile_image_url\" : \"http://pbs.twimg.com/profile_images/378800000157469481/0a267258c8ccd1bf53d01c115677dbd7_normal.jpeg\",\n      \"profile_image_url_https\" : \"https://pbs.twimg.com/profile_images/378800000157469481/0a267258c8ccd1bf53d01c115677dbd7_normal.jpeg\",\n      \"profile_link_color\" : \"0099CC\",\n      \"profile_sidebar_border_color\" : \"FFF8AD\",\n      \"profile_sidebar_fill_color\" : \"F6FFD1\",\n      \"profile_text_color\" : \"333333\",\n      \"profile_use_background_image\" : true,\n      \"protected\" : false,\n      \"screen_name\" : \"nineemperor1\",\n      \"statuses_count\" : 9652,\n      \"time_zone\" : null,\n      \"url\" : null,\n      \"utc_offset\" : null,\n      \"verified\" : false\n    }\n}\n```\n\nIn order to extract a field from a JSON document, we can use the `get_json_object` function.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170218-161837_1521722724","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>3. Counting Hash-Tags</h1>\n<p>So we now want to create a streaming hashtag count. First we need to extract the Tweet itself from the JSON document, then we need to extract the hashtags in a similar way to the batch word traditional DataFrame word count example, i.e. we split every line into words, keep only hash-tags, group the words and count the sizes of the groups.</p>\n<p>Each query looks as follows</p>\n<pre><code>{ \"contributors\" : null,\n  \"coordinates\" : null,\n  \"created_at\" : \"Fri Jul 29 12:46:00 +0000 2016\",\n  \"entities\" : { \"hashtags\" : [  ],\n      \"symbols\" : [  ],\n      \"urls\" : [ { \"display_url\" : \"fb.me/ItnwZEhy\",\n            \"expanded_url\" : \"http://fb.me/ItnwZEhy\",\n            \"indices\" : [ 33,\n                56\n              ],\n            \"url\" : \"https://t.co/mM0if95F1K\"\n          } ],\n      \"user_mentions\" : [  ]\n    },\n  \"favorite_count\" : 0,\n  \"favorited\" : false,\n  \"filter_level\" : \"low\",\n  \"geo\" : null,\n  \"id\" : 759007065155117058,\n  \"id_str\" : \"759007065155117058\",\n  \"in_reply_to_screen_name\" : null,\n  \"in_reply_to_status_id\" : null,\n  \"in_reply_to_status_id_str\" : null,\n  \"in_reply_to_user_id\" : null,\n  \"in_reply_to_user_id_str\" : null,\n  \"is_quote_status\" : false,\n  \"lang\" : \"en\",\n  \"place\" : null,\n  \"possibly_sensitive\" : false,\n  \"retweet_count\" : 0,\n  \"retweeted\" : false,\n  \"source\" : \"&lt;a href=\\\"http://www.facebook.com/twitter\\\" rel=\\\"nofollow\\\"&gt;Facebook&lt;/a&gt;\",\n  \"text\" : \"I posted a new video to Facebook https://t.co/mM0if95F1K\",\n  \"timestamp_ms\" : \"1469796360659\",\n  \"truncated\" : false,\n  \"user\" : { \"contributors_enabled\" : false,\n      \"created_at\" : \"Sat Sep 08 08:28:55 +0000 2012\",\n      \"default_profile\" : false,\n      \"default_profile_image\" : false,\n      \"description\" : null,\n      \"favourites_count\" : 0,\n      \"follow_request_sent\" : null,\n      \"followers_count\" : 0,\n      \"following\" : null,\n      \"friends_count\" : 0,\n      \"geo_enabled\" : false,\n      \"id\" : 810489374,\n      \"id_str\" : \"810489374\",\n      \"is_translator\" : false,\n      \"lang\" : \"zh-tw\",\n      \"listed_count\" : 0,\n      \"location\" : null,\n      \"name\" : \"張冥閻\",\n      \"notifications\" : null,\n      \"profile_background_color\" : \"FFF04D\",\n      \"profile_background_image_url\" : \"http://abs.twimg.com/images/themes/theme19/bg.gif\",\n      \"profile_background_image_url_https\" : \"https://abs.twimg.com/images/themes/theme19/bg.gif\",\n      \"profile_background_tile\" : false,\n      \"profile_image_url\" : \"http://pbs.twimg.com/profile_images/378800000157469481/0a267258c8ccd1bf53d01c115677dbd7_normal.jpeg\",\n      \"profile_image_url_https\" : \"https://pbs.twimg.com/profile_images/378800000157469481/0a267258c8ccd1bf53d01c115677dbd7_normal.jpeg\",\n      \"profile_link_color\" : \"0099CC\",\n      \"profile_sidebar_border_color\" : \"FFF8AD\",\n      \"profile_sidebar_fill_color\" : \"F6FFD1\",\n      \"profile_text_color\" : \"333333\",\n      \"profile_use_background_image\" : true,\n      \"protected\" : false,\n      \"screen_name\" : \"nineemperor1\",\n      \"statuses_count\" : 9652,\n      \"time_zone\" : null,\n      \"url\" : null,\n      \"utc_offset\" : null,\n      \"verified\" : false\n    }\n}\n</code></pre>\n<p>In order to extract a field from a JSON document, we can use the <code>get_json_object</code> function.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7185"},{"text":"%md\n## 3.1 Extract Tweet\n\nFirst we need to extract the tweet text itself via the `get_json_object` function and store it into a new column.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170220-182415_1872492795","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>3.1 Extract Tweet</h2>\n<p>First we need to extract the tweet text itself via the <code>get_json_object</code> function and store it into a new column.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7186"},{"text":"val ts_text = ...","dateUpdated":"2017-02-20T18:32:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170218-160238_1475524112","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7187","focus":true},{"text":"%md\n## 3.2 Extract Topics\n\nNow that we have the Tweet text itself, we split it along spaces and extract all hash-tags.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170220-182404_1394954195","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>3.2 Extract Topics</h2>\n<p>Now that we have the Tweet text itself, we split it along spaces and extract all hash-tags.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7188"},{"text":"val topics = ...","dateUpdated":"2017-02-20T18:32:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170220-180159_930749179","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7189","focus":true},{"text":"%md\n## 3.3 Count Topics\n\nNow that we have the hash tags (topics), we perform a simple aggregation as usual.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170220-182554_1623876741","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>3.3 Count Topics</h2>\n<p>Now that we have the hash tags (topics), we perform a simple aggregation as usual.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7190"},{"text":"val counts = ...","dateUpdated":"2017-02-20T18:32:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170218-160526_2008806660","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7191","focus":true},{"text":"counts.printSchema","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170218-162308_1835168589","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7192","focus":true},{"text":"%md\n# 4. Print Results onto Console\n\nAgain we want to print the results onto the console.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170218-161041_735302634","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>4. Print Results onto Console</h1>\n<p>Again we want to print the results onto the console.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7193"},{"text":"val query = ...","dateUpdated":"2017-02-20T18:32:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402898_262311893","id":"20170218-160613_988660270","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7194","focus":true},{"text":"","dateUpdated":"2017-02-20T18:32:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402899_261927145","id":"20170218-160716_194527039","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7195"},{"text":"%md\n# 5. Time-Windowed Aggregation\n\nAnother interesting (and probably more realistic) application is to perform time windowed aggregations. This means that we define a sliding time window used in the `groupBy` clause. In addition we also define a so called *watermark* which tells Spark how long to wait for late arrivels of individual data points (we don't have them in our simple example).","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402899_261927145","id":"20170218-164639_1471935006","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>5. Time-Windowed Aggregation</h1>\n<p>Another interesting (and probably more realistic) application is to perform time windowed aggregations. This means that we define a sliding time window used in the <code>groupBy</code> clause. In addition we also define a so called <em>watermark</em> which tells Spark how long to wait for late arrivels of individual data points (we don't have them in our simple example).</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7196"},{"text":"val windowedCounts = ...","dateUpdated":"2017-02-20T18:32:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402899_261927145","id":"20170218-164703_2039341008","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7197","focus":true},{"text":"windowedCounts.printSchema()","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402899_261927145","id":"20170218-165357_1483843188","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7198","focus":true},{"text":"%md\n# 5. Create Dynamic Table\n\nWe can also use a \"memory\" output, which is a queryable live table. In order to do so, we again create a new table, but this time with format `memory` and an explicit query name `topic_counts`. Using a `memory` output will create a dynamic table in memory (only `complete` output supported right now), which can be queried using SQL.\n\n1. Create a DataStreamWriter object using the writeStream method of your DataFrame `windowedCounts`.\n2. Set the format to `memory`\n3. Set the output mode to `append` (this is supported for time windowed aggregations)\n4. Set the query name to `topic_counts`\n5. Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)\n6. Start the continuous query via `start`","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402899_261927145","id":"20170218-160934_1425298948","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>5. Create Dynamic Table</h1>\n<p>We can also use a &ldquo;memory&rdquo; output, which is a queryable live table. In order to do so, we again create a new table, but this time with format <code>memory</code> and an explicit query name <code>topic_counts</code>. Using a <code>memory</code> output will create a dynamic table in memory (only <code>complete</code> output supported right now), which can be queried using SQL.</p>\n<ol>\n<li>Create a DataStreamWriter object using the writeStream method of your DataFrame <code>windowedCounts</code>.</li>\n<li>Set the format to <code>memory</code></li>\n<li>Set the output mode to <code>append</code> (this is supported for time windowed aggregations)</li>\n<li>Set the query name to <code>topic_counts</code></li>\n<li>Specify a checkPointLocation on HDFS (ok, this is not trivial, so it is in the code below)</li>\n<li>Start the continuous query via <code>start</code></li>\n</ol>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7199"},{"text":"val tableQuery = ...","dateUpdated":"2017-02-20T18:32:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402899_261927145","id":"20170218-161145_208429016","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7200","focus":true},{"text":"%md\n## 5.1 Perform Query\n\nNow that we have a dynamic table, we can perform SQL queries against this table as if it was a normal static table.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402899_261927145","id":"20170218-162010_1110788275","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>5.1 Perform Query</h2>\n<p>Now that we have a dynamic table, we can perform SQL queries against this table as if it was a normal static table.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7201"},{"text":"%sql\n","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"window","index":0,"aggr":"sum"}],"values":[{"name":"topic","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"window","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402899_261927145","id":"20170218-161337_1103351127","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7202","focus":true},{"text":"%md\n## 5.2 Stop Query\n\nIn order to clean everything up, we stop the query again.","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402899_261927145","id":"20170218-162048_842521010","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>5.2 Stop Query</h2>\n<p>In order to clean everything up, we stop the query again.</p>\n"},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7203"},{"text":"","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402899_261927145","id":"20170218-161329_1844676678","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7204"},{"text":"","dateUpdated":"2017-02-20T18:30:02+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487615402900_260003400","id":"20170218-162247_324928954","dateCreated":"2017-02-20T18:30:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7205"}],"name":"Twitter Structured Streaming - Skeleton","id":"2CCASNE5J","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}